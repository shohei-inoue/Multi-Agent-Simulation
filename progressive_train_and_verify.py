"""
æ®µéšçš„å­¦ç¿’ã¨æ¤œè¨¼å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å„æ®µéšã§å‰ã®æ§‹æˆã®æ¤œè¨¼ã‚’è¡Œã„ãªãŒã‚‰ã€æ¬¡ã®æ§‹æˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹
"""

import os
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

from core.application import Application
from params.simulation import SimulationParam
from params.agent import AgentParam
from params.system_agent import SystemAgentParam, LearningParameter as SystemLearningParameter
from params.swarm_agent import SwarmAgentParam, LearningParameter as SwarmLearningParameter
from agents.agent_factory import create_initial_agents
from envs.env import Env


@dataclass
class ProgressiveConfig:
    """æ®µéšçš„è¨­å®š"""
    name: str
    system_agent_learning: bool
    swarm_agent_learning: bool
    system_agent_branching: bool
    description: str
    use_pretrained_models: bool = False
    num_episodes: int = 50  # æ¤œè¨¼ç”¨ã«çŸ­ç¸®
    max_steps_per_episode: int = 200
    target_exploration_rate: float = 0.8
    num_runs: int = 3  # æ¤œè¨¼ç”¨ã«çŸ­ç¸®
    
    def __str__(self):
        return f"{self.name}_SysL{self.system_agent_learning}_SwarmL{self.swarm_agent_learning}_Branch{self.system_agent_branching}"


@dataclass
class EnvironmentConfig:
    """ç’°å¢ƒè¨­å®š"""
    map_width: int = 100
    map_height: int = 200
    obstacle_density: float = 0.0
    robot_count: int = 20
    
    def __str__(self):
        return f"Map{self.map_width}x{self.map_height}_Obs{self.obstacle_density}_Robot{self.robot_count}"


class ProgressiveTrainAndVerifyRunner:
    """æ®µéšçš„å­¦ç¿’ã¨æ¤œè¨¼ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_config = SimulationParam()
        self.training_results = {}
        self.verification_results = {}
        
    def create_progressive_configs(self) -> List[ProgressiveConfig]:
        """æ®µéšçš„è¨­å®šã‚’ä½œæˆ"""
        configs = [
            ProgressiveConfig(
                name='Config_A',
                system_agent_learning=False,
                swarm_agent_learning=False,
                system_agent_branching=False,
                description='SystemAgent(å­¦ç¿’ãªã—) + SwarmAgent(å­¦ç¿’ãªã—)',
                use_pretrained_models=False
            ),
            ProgressiveConfig(
                name='Config_B',
                system_agent_learning=False,
                swarm_agent_learning=True,
                system_agent_branching=False,
                description='SystemAgent(å­¦ç¿’ãªã—) + SwarmAgent(å­¦ç¿’ã‚ã‚Š)',
                use_pretrained_models=True
            ),
            ProgressiveConfig(
                name='Config_C',
                system_agent_learning=False,
                swarm_agent_learning=False,
                system_agent_branching=True,
                description='SystemAgent(å­¦ç¿’ãªã—) + SwarmAgent(å­¦ç¿’ãªã—) + åˆ†å²ãƒ»çµ±åˆ',
                use_pretrained_models=False
            ),
            ProgressiveConfig(
                name='Config_D',
                system_agent_learning=True,
                swarm_agent_learning=True,
                system_agent_branching=True,
                description='SystemAgent(å­¦ç¿’ã‚ã‚Š) + SwarmAgent(å­¦ç¿’ã‚ã‚Š) + åˆ†å²ãƒ»çµ±åˆ',
                use_pretrained_models=True
            )
        ]
        return configs
    
    def create_environment_configs(self) -> List[EnvironmentConfig]:
        """ç’°å¢ƒè¨­å®šã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        configs = [
            EnvironmentConfig(map_width=100, map_height=200, obstacle_density=0.0, robot_count=20),
            EnvironmentConfig(map_width=100, map_height=200, obstacle_density=0.003, robot_count=20),
            EnvironmentConfig(map_width=100, map_height=200, obstacle_density=0.005, robot_count=20),
        ]
        return configs
    
    def setup_training_environment(self) -> SimulationParam:
        """å­¦ç¿’ç”¨ç’°å¢ƒè¨­å®š"""
        sim_param = self.base_config.copy()
        
        # å­¦ç¿’ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        sim_param.environment.map.width = 200
        sim_param.environment.map.height = 100
        sim_param.environment.obstacle.probability = 0.0  # å­¦ç¿’æ™‚ã¯éšœå®³ç‰©ãªã—
        sim_param.explore.robotNum = 20
        sim_param.explore.coordinate.x = 50.0
        sim_param.explore.coordinate.y = 100.0
        sim_param.explore.boundary.outer = 5.0
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        sim_param.agent.episodeNum = 1500
        sim_param.agent.maxStepsPerEpisode = 50
        
        return sim_param
    
    def setup_verification_environment(self, env_config: EnvironmentConfig) -> SimulationParam:
        """æ¤œè¨¼ç”¨ç’°å¢ƒè¨­å®š"""
        sim_param = self.base_config.copy()
        
        # ãƒãƒƒãƒ—ã‚µã‚¤ã‚ºè¨­å®š
        sim_param.environment.map.width = env_config.map_width
        sim_param.environment.map.height = env_config.map_height
        
        # éšœå®³ç‰©å¯†åº¦è¨­å®š
        sim_param.environment.obstacle.probability = env_config.obstacle_density
        
        # ãƒ­ãƒœãƒƒãƒˆæ•°è¨­å®š
        sim_param.explore.robotNum = env_config.robot_count
        
        # ãƒ­ã‚°è¨­å®šï¼ˆGIFç”Ÿæˆç”¨ï¼‰
        sim_param.robot_logging.save_robot_data = True
        sim_param.robot_logging.save_position = True
        sim_param.robot_logging.save_collision = True
        sim_param.robot_logging.sampling_rate = 1.0
        
        return sim_param
    
    def setup_agent_config(self, config: ProgressiveConfig, is_training: bool = True) -> AgentParam:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã‚’ä½œæˆ"""
        agent_param = AgentParam()
        
        # SystemAgentè¨­å®š
        system_param = SystemAgentParam()
        if config.system_agent_learning:
            system_param.learningParameter = SystemLearningParameter()
        else:
            system_param.learningParameter = None
        
        # åˆ†å²ãƒ»çµ±åˆè¨­å®š
        if config.system_agent_branching:
            system_param.branch_condition.branch_enabled = True
            system_param.integration_condition.integration_enabled = True
        else:
            system_param.branch_condition.branch_enabled = False
            system_param.integration_condition.integration_enabled = False
        
        agent_param.system_agent_param = system_param
        
        # SwarmAgentè¨­å®š
        swarm_param = SwarmAgentParam()
        if config.swarm_agent_learning:
            swarm_param.isLearning = True
            swarm_param.learningParameter = SwarmLearningParameter()
        else:
            swarm_param.isLearning = False
            swarm_param.learningParameter = None
        
        agent_param.swarm_agent_params = [swarm_param]
        
        return agent_param
    
    def train_models(self, config: ProgressiveConfig) -> Dict:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Ÿè¡Œ"""
        print(f"å­¦ç¿’é–‹å§‹: {config.name} - {config.description}")
        
        # ç’°å¢ƒè¨­å®š
        sim_param = self.setup_training_environment()
        env = Env(sim_param)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
        agent_param = self.setup_agent_config(config, is_training=True)
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        system_agent, swarm_agents = create_initial_agents(env, agent_param)
        
        # å­¦ç¿’å®Ÿè¡Œ
        training_results = self._execute_training(env, system_agent, swarm_agents, config)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self._save_models(system_agent, swarm_agents, config)
        
        return training_results
    
    def _execute_training(self, env, system_agent, swarm_agents, config: ProgressiveConfig) -> Dict:
        """å­¦ç¿’å®Ÿè¡Œ"""
        episode_results = []
        
        for episode in range(self.base_config.agent.episodeNum):
            state = env.reset()
            episode_reward = 0.0
            episode_exploration = 0.0
            
            for step in range(self.base_config.agent.maxStepsPerEpisode):
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•æ±ºå®š
                if config.system_agent_learning:
                    system_action = system_agent.get_action(state, episode)
                else:
                    system_action = self._get_default_system_action()
                
                if config.swarm_agent_learning:
                    swarm_actions = {}
                    for swarm_id, agent in swarm_agents.items():
                        swarm_state = env.get_swarm_agent_observation(swarm_id)
                        swarm_actions[swarm_id] = agent.get_action(swarm_state, episode)
                else:
                    swarm_actions = {swarm_id: self._get_default_swarm_action() 
                                   for swarm_id in swarm_agents.keys()}
                
                # ç’°å¢ƒæ›´æ–°
                next_state, reward, done, truncated, info = env.step(swarm_actions)
                
                episode_reward += reward
                episode_exploration = env.get_exploration_rate()
                
                state = next_state
                
                if done or truncated:
                    break
            
            # å­¦ç¿’æ›´æ–°
            if config.system_agent_learning:
                system_agent.train()
            if config.swarm_agent_learning:
                for agent in swarm_agents.values():
                    agent.train()
            
            # çµæœè¨˜éŒ²
            episode_results.append({
                'episode': episode,
                'reward': episode_reward,
                'exploration_rate': episode_exploration,
                'steps': step + 1
            })
            
            if episode % 100 == 0:
                print(f"  Episode {episode}: Reward={episode_reward:.2f}, Exploration={episode_exploration:.3f}")
        
        return {
            'config': config,
            'episode_results': episode_results,
            'final_exploration_rate': episode_exploration,
            'avg_reward': sum(r['reward'] for r in episode_results) / len(episode_results)
        }
    
    def _save_models(self, system_agent, swarm_agents, config: ProgressiveConfig):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""
        models_dir = "trained_models"
        os.makedirs(models_dir, exist_ok=True)
        
        config_dir = os.path.join(models_dir, config.name)
        os.makedirs(config_dir, exist_ok=True)
        
        # SystemAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if config.system_agent_learning and system_agent and hasattr(system_agent, 'model'):
            system_model_path = os.path.join(config_dir, "system_agent_model.keras")
            system_agent.model.save(system_model_path)
            print(f"  SystemAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜: {system_model_path}")
        
        # SwarmAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜
        if config.swarm_agent_learning and swarm_agents:
            swarm_model_path = os.path.join(config_dir, "swarm_agent_model.keras")
            # æœ€åˆã®SwarmAgentã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆå…¨SwarmAgentã¯åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
            first_agent = list(swarm_agents.values())[0]
            if hasattr(first_agent, 'model'):
                first_agent.model.save(swarm_model_path)
                print(f"  SwarmAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜: {swarm_model_path}")
        
        # å­¦ç¿’è¨­å®šä¿å­˜
        config_path = os.path.join(config_dir, "training_config.json")
        with open(config_path, 'w') as f:
            json.dump({
                'name': config.name,
                'system_agent_learning': config.system_agent_learning,
                'swarm_agent_learning': config.swarm_agent_learning,
                'system_agent_branching': config.system_agent_branching,
                'description': config.description
            }, f, indent=2)
    
    def run_verification(self, config: ProgressiveConfig, 
                        env_config: EnvironmentConfig) -> Dict[str, Any]:
        """æ¤œè¨¼å®Ÿè¡Œ"""
        print(f"æ¤œè¨¼å®Ÿè¡Œä¸­: {config.name} - {env_config}")
        
        # è¤‡æ•°å›å®Ÿè¡Œã®çµæœã‚’æ ¼ç´
        run_results = []
        
        for run in range(config.num_runs):
            print(f"  å®Ÿè¡Œ {run + 1}/{config.num_runs}")
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            output_dir = f"verification_results/{config.name}_{env_config}_{run+1}"
            os.makedirs(output_dir, exist_ok=True)
            
            # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–
            sim_param = self.setup_verification_environment(env_config)
            env = Env(sim_param)
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
            agent_param = self.setup_agent_config(config, is_training=False)
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            system_agent, swarm_agents = create_initial_agents(env, agent_param)
            
            # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
            self.load_pretrained_models(system_agent, swarm_agents, config)
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ
            run_result = self._execute_verification_episodes(env, system_agent, swarm_agents, config, output_dir)
            run_results.append(run_result)
        
        # è¤‡æ•°å›å®Ÿè¡Œã®çµ±è¨ˆåˆ†æ
        return self._analyze_multiple_runs(run_results, config)
    
    def load_pretrained_models(self, system_agent, swarm_agents, config: ProgressiveConfig):
        """äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not config.use_pretrained_models:
            return
        
        # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        models_dir = "trained_models"
        
        try:
            # å¯¾å¿œã™ã‚‹å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
            if config.name == "Config_B":
                model_config_name = "Config_B"
            elif config.name == "Config_D":
                model_config_name = "Config_D"
            else:
                print(f"Warning: No pretrained model for {config.name}")
                return
            
            config_dir = os.path.join(models_dir, model_config_name)
            
            # SystemAgentã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            if config.system_agent_learning and system_agent:
                system_model_path = os.path.join(config_dir, "system_agent_model.keras")
                if os.path.exists(system_model_path):
                    system_agent.model.load_weights(system_model_path)
                    print(f"Loaded pretrained SystemAgent model from {system_model_path}")
            
            # SwarmAgentã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
            if config.swarm_agent_learning and swarm_agents:
                swarm_model_path = os.path.join(config_dir, "swarm_agent_model.keras")
                if os.path.exists(swarm_model_path):
                    for agent in swarm_agents.values():
                        agent.model.load_weights(swarm_model_path)
                    print(f"Loaded pretrained SwarmAgent model from {swarm_model_path}")
                    
        except Exception as e:
            print(f"Warning: Could not load pretrained models: {e}")
            print("Continuing with randomly initialized models")
    
    def _execute_verification_episodes(self, env, system_agent, swarm_agents, 
                                     config: ProgressiveConfig, output_dir: str) -> Dict[str, Any]:
        """æ¤œè¨¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å®Ÿè¡Œ"""
        episode_results = []
        target_reached_episodes = []
        
        for episode in range(config.num_episodes):
            env.reset()
            episode_data = {
                'episode': episode,
                'exploration_rates': [],
                'steps_to_target': None,
                'final_exploration_rate': 0.0
            }
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹
            env.start_episode(episode)
            
            for step in range(config.max_steps_per_episode):
                # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
                state = env.get_state()
                
                # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•æ±ºå®š
                if config.system_agent_learning:
                    system_action = system_agent.get_action(state, episode)
                else:
                    system_action = self._get_default_system_action()
                
                if config.swarm_agent_learning:
                    swarm_actions = {swarm_id: agent.get_action(state, episode) for swarm_id, agent in swarm_agents.items()}
                else:
                    swarm_actions = {swarm_id: self._get_default_swarm_action() for swarm_id in swarm_agents.keys()}
                
                # ç’°å¢ƒæ›´æ–°
                next_state, reward, done, truncated, info = env.step(swarm_actions)
                
                # ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                exploration_rate = env.get_exploration_rate()
                episode_data['exploration_rates'].append(exploration_rate)
                
                # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                if exploration_rate >= config.target_exploration_rate:
                    episode_data['steps_to_target'] = step + 1
                    target_reached_episodes.append(episode)
                    break
            
            episode_data['final_exploration_rate'] = episode_data['exploration_rates'][-1]
            episode_results.append(episode_data)
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ã¨GIFä¿å­˜
            env.end_episode(output_dir)
        
        # çµæœé›†è¨ˆ
        return self._aggregate_results(episode_results, target_reached_episodes)
    
    def _get_default_system_action(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®SystemAgentè¡Œå‹•"""
        return {
            'action_type': 'none',
            'target_swarm_id': 0,
            'branch_threshold': 0.3,
            'integration_threshold': 0.7
        }
    
    def _get_default_swarm_action(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®SwarmAgentè¡Œå‹•"""
        return {
            'theta': np.random.uniform(0, 2*np.pi),
            'th': 0.5,
            'k_e': 10.0,
            'k_c': 5.0
        }
    
    def _aggregate_results(self, episode_results: List[Dict], 
                          target_reached_episodes: List[int]) -> Dict[str, Any]:
        """çµæœã‚’é›†è¨ˆ"""
        # ç›®æ¨™é”æˆç‡
        target_reach_rate = len(target_reached_episodes) / len(episode_results)
        
        # å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆç›®æ¨™é”æˆæ™‚ï¼‰
        steps_to_target = [ep['steps_to_target'] for ep in episode_results 
                          if ep['steps_to_target'] is not None]
        avg_steps_to_target = np.mean(steps_to_target) if steps_to_target else None
        
        # æ¢æŸ»é€²æ—é€Ÿåº¦
        exploration_speeds = []
        for ep_data in episode_results:
            if len(ep_data['exploration_rates']) > 1:
                speed = (ep_data['final_exploration_rate'] - ep_data['exploration_rates'][0]) / len(ep_data['exploration_rates'])
                exploration_speeds.append(speed)
        
        avg_exploration_speed = np.mean(exploration_speeds) if exploration_speeds else 0.0
        
        # æœ€çµ‚æ¢æŸ»ç‡
        final_exploration_rates = [ep['final_exploration_rate'] for ep in episode_results]
        avg_final_exploration_rate = np.mean(final_exploration_rates)
        
        return {
            'target_reach_rate': target_reach_rate,
            'avg_steps_to_target': avg_steps_to_target,
            'avg_exploration_speed': avg_exploration_speed,
            'avg_final_exploration_rate': avg_final_exploration_rate,
            'episode_results': episode_results,
            'target_reached_episodes': target_reached_episodes
        }
    
    def _analyze_multiple_runs(self, run_results: List[Dict], config: ProgressiveConfig) -> Dict[str, Any]:
        """è¤‡æ•°å›å®Ÿè¡Œã®çµæœã‚’çµ±è¨ˆåˆ†æ"""
        # å„æŒ‡æ¨™ã®çµ±è¨ˆã‚’è¨ˆç®—
        target_reach_rates = [r['target_reach_rate'] for r in run_results]
        avg_steps_to_targets = [r['avg_steps_to_target'] for r in run_results if r['avg_steps_to_target'] is not None]
        avg_exploration_speeds = [r['avg_exploration_speed'] for r in run_results]
        final_exploration_rates = [r['final_exploration_rate'] for r in run_results]
        
        # çµ±è¨ˆé‡ã®è¨ˆç®—
        stats = {
            'target_reach_rate': {
                'mean': np.mean(target_reach_rates),
                'std': np.std(target_reach_rates),
                'min': np.min(target_reach_rates),
                'max': np.max(target_reach_rates),
                'values': target_reach_rates
            },
            'avg_steps_to_target': {
                'mean': np.mean(avg_steps_to_targets) if avg_steps_to_targets else None,
                'std': np.std(avg_steps_to_targets) if avg_steps_to_targets else None,
                'min': np.min(avg_steps_to_targets) if avg_steps_to_targets else None,
                'max': np.max(avg_steps_to_targets) if avg_steps_to_targets else None,
                'values': avg_steps_to_targets
            },
            'avg_exploration_speed': {
                'mean': np.mean(avg_exploration_speeds),
                'std': np.std(avg_exploration_speeds),
                'min': np.min(avg_exploration_speeds),
                'max': np.max(avg_exploration_speeds),
                'values': avg_exploration_speeds
            },
            'final_exploration_rate': {
                'mean': np.mean(final_exploration_rates),
                'std': np.std(final_exploration_rates),
                'min': np.min(final_exploration_rates),
                'max': np.max(final_exploration_rates),
                'values': final_exploration_rates
            },
            'num_runs': config.num_runs
        }
        
        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€å¾“æ¥ã®å½¢å¼ã‚‚ä¿æŒ
        stats['target_reach_rate'] = stats['target_reach_rate']['mean']
        stats['avg_steps_to_target'] = stats['avg_steps_to_target']['mean']
        stats['avg_exploration_speed'] = stats['avg_exploration_speed']['mean']
        stats['final_exploration_rate'] = stats['final_exploration_rate']['mean']
        
        return stats
    
    def run_progressive_experiments(self):
        """æ®µéšçš„å®Ÿé¨“å®Ÿè¡Œ"""
        print("=== æ®µéšçš„å­¦ç¿’ã¨æ¤œè¨¼ã®å®Ÿè¡Œé–‹å§‹ ===")
        
        # è¨­å®šã‚’å–å¾—
        configs = self.create_progressive_configs()
        environment_configs = self.create_environment_configs()
        
        # æ®µéšçš„å®Ÿè¡Œ
        for i, current_config in enumerate(configs):
            print(f"\n{'='*50}")
            print(f"æ®µéš {i+1}: {current_config.name}")
            print(f"{'='*50}")
            
            # 1. å‰ã®æ§‹æˆã®æ¤œè¨¼ï¼ˆæœ€åˆã®æ®µéšä»¥å¤–ï¼‰
            if i > 0:
                print(f"\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—1: {configs[i-1].name} ã®æ¤œè¨¼")
                prev_config = configs[i-1]
                exp_results = {}
                
                for env_config in environment_configs:
                    try:
                        result = self.run_verification(prev_config, env_config)
                        exp_results[str(env_config)] = result
                        print(f"æ¤œè¨¼å®Œäº†: {prev_config.name} - {env_config}")
                    except Exception as e:
                        print(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {prev_config.name} - {env_config}: {e}")
                
                self.verification_results[str(prev_config)] = exp_results
            
            # 2. ç¾åœ¨ã®æ§‹æˆã®å­¦ç¿’ï¼ˆå­¦ç¿’ãŒå¿…è¦ãªå ´åˆï¼‰
            if current_config.swarm_agent_learning or current_config.system_agent_learning:
                print(f"\nğŸ“š ã‚¹ãƒ†ãƒƒãƒ—2: {current_config.name} ã®å­¦ç¿’")
                try:
                    result = self.train_models(current_config)
                    self.training_results[current_config.name] = result
                    print(f"å­¦ç¿’å®Œäº†: {current_config.name}")
                except Exception as e:
                    print(f"å­¦ç¿’ã‚¨ãƒ©ãƒ¼ {current_config.name}: {e}")
            else:
                print(f"\nğŸ“š ã‚¹ãƒ†ãƒƒãƒ—2: {current_config.name} ã¯å­¦ç¿’ä¸è¦ï¼ˆå­¦ç¿’ãªã—æ§‹æˆï¼‰")
        
        # æœ€å¾Œã®æ§‹æˆã®æ¤œè¨¼
        print(f"\nğŸ” æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—: {configs[-1].name} ã®æ¤œè¨¼")
        final_config = configs[-1]
        exp_results = {}
        
        for env_config in environment_configs:
            try:
                result = self.run_verification(final_config, env_config)
                exp_results[str(env_config)] = result
                print(f"æ¤œè¨¼å®Œäº†: {final_config.name} - {env_config}")
            except Exception as e:
                print(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {final_config.name} - {env_config}: {e}")
        
        self.verification_results[str(final_config)] = exp_results
        
        # çµæœä¿å­˜
        self.save_all_results()
        
        print("\nğŸ‰ === æ®µéšçš„å­¦ç¿’ã¨æ¤œè¨¼ã®å®Ÿè¡Œå®Œäº† ===")
    
    def save_all_results(self):
        """å…¨çµæœä¿å­˜"""
        # å­¦ç¿’çµæœä¿å­˜
        self.save_training_results()
        
        # æ¤œè¨¼çµæœä¿å­˜
        self.save_verification_results()
    
    def save_training_results(self):
        """å­¦ç¿’çµæœä¿å­˜"""
        results_dir = "training_results"
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = os.path.join(results_dir, f"progressive_training_results_{timestamp}.json")
        
        # JSONå¤‰æ›å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
        serializable_results = {}
        for name, result in self.training_results.items():
            serializable_results[name] = {
                'config': {
                    'name': result['config'].name,
                    'system_agent_learning': result['config'].system_agent_learning,
                    'swarm_agent_learning': result['config'].swarm_agent_learning,
                    'system_agent_branching': result['config'].system_agent_branching,
                    'description': result['config'].description
                },
                'final_exploration_rate': result['final_exploration_rate'],
                'avg_reward': result['avg_reward'],
                'episode_count': len(result['episode_results'])
            }
        
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"å­¦ç¿’çµæœä¿å­˜: {results_file}")
    
    def save_verification_results(self):
        """æ¤œè¨¼çµæœä¿å­˜"""
        results_dir = "verification_results"
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = os.path.join(results_dir, f"progressive_verification_results_{timestamp}.json")
        
        with open(results_file, 'w') as f:
            json.dump(self.verification_results, f, indent=2, default=str)
        
        print(f"æ¤œè¨¼çµæœä¿å­˜: {results_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = ProgressiveTrainAndVerifyRunner()
    runner.run_progressive_experiments()


if __name__ == "__main__":
    main() 