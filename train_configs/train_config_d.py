#!/usr/bin/env python3
"""
Config_D å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
SystemAgent: å­¦ç¿’ã‚ã‚Šã€åˆ†å²ãƒ»çµ±åˆã‚ã‚Š
SwarmAgent: å­¦ç¿’ã‚ã‚Š
"""

import os
import sys
import numpy as np
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def setup_training_environment():
    """å­¦ç¿’ç”¨ç’°å¢ƒè¨­å®š"""
    from params.simulation import SimulationParam
    
    sim_param = SimulationParam()
    
    # åŸºæœ¬è¨­å®š
    sim_param.episodeNum = 1000  # å­¦ç¿’ç”¨ã«1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
    sim_param.maxStepsPerEpisode = 100
    
    # ç’°å¢ƒè¨­å®š
    sim_param.environment.map.width = 200
    sim_param.environment.map.height = 100
    sim_param.environment.obstacle.probability = 0.0
    
    # æŽ¢æŸ»è¨­å®š
    sim_param.explore.robotNum = 20
    sim_param.explore.coordinate.x = 10.0
    sim_param.explore.coordinate.y = 10.0
    sim_param.explore.boundary.inner = 0.0
    sim_param.explore.boundary.outer = 20.0
    
    # ãƒ­ã‚°è¨­å®šï¼ˆå­¦ç¿’ä¸­ã¯ç„¡åŠ¹åŒ–ï¼‰
    sim_param.robot_logging.save_robot_data = False
    sim_param.robot_logging.save_position = False
    sim_param.robot_logging.save_collision = False
    sim_param.robot_logging.sampling_rate = 1.0
    
    return sim_param

def setup_config_d_agent():
    """Config_Dç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š"""
    from params.agent import AgentParam
    from params.system_agent import SystemAgentParam
    from params.swarm_agent import SwarmAgentParam
    from params.learning import LearningParameter
    
    agent_param = AgentParam()
    
    # SystemAgentè¨­å®šï¼ˆå­¦ç¿’ã‚ã‚Šï¼‰
    system_param = SystemAgentParam()
    system_param.isLearning = True
    system_param.learningParameter = LearningParameter(
        type="A2C",
        model="actor-critic",  # ãƒ¢ãƒ‡ãƒ«ã¯å¾Œã§è¨­å®š
        optimizer="adam",  # ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã¯å¾Œã§è¨­å®š
        gamma=0.99,
        learningLate=0.001,
        nStep=5
    )
    agent_param.system_agent_param = system_param
    
    # SwarmAgentè¨­å®šï¼ˆå­¦ç¿’ã‚ã‚Šï¼‰
    swarm_param = SwarmAgentParam()
    swarm_param.isLearning = True
    swarm_param.learningParameter = LearningParameter(
        type="A2C",
        model=None,  # ãƒ¢ãƒ‡ãƒ«ã¯å¾Œã§è¨­å®š
        optimizer=None,  # ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã¯å¾Œã§è¨­å®š
        gamma=0.99,
        learningLate=0.001,
        nStep=5
    )
    agent_param.swarm_agent_params = [swarm_param]
    
    return agent_param

def run_training():
    """å­¦ç¿’å®Ÿè¡Œ"""
    print("=== Config_D å­¦ç¿’é–‹å§‹ ===")
    
    try:
        # 1. ç’°å¢ƒè¨­å®š
        print("1. ç’°å¢ƒè¨­å®šä¸­...")
        sim_param = setup_training_environment()
        print("âœ“ ç’°å¢ƒè¨­å®šå®Œäº†")
        
        # 2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
        print("2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šä¸­...")
        agent_param = setup_config_d_agent()
        print("âœ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šå®Œäº†")
        
        # 3. ç’°å¢ƒä½œæˆ
        print("3. ç’°å¢ƒä½œæˆä¸­...")
        from envs.env import Env
        env = Env(sim_param)
        print("âœ“ ç’°å¢ƒä½œæˆå®Œäº†")
        
        # 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        print("4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆä¸­...")
        from agents.agent_factory import create_initial_agents
        system_agent, swarm_agents = create_initial_agents(env, agent_param)
        print(f"âœ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆå®Œäº† - SwarmAgents: {len(swarm_agents)}")
        
        # 5. SystemAgentã‚’ç’°å¢ƒã«è¨­å®š
        print("5. SystemAgentã‚’ç’°å¢ƒã«è¨­å®šä¸­...")
        env.set_system_agent(system_agent)
        print("âœ“ SystemAgentè¨­å®šå®Œäº†")
        
        # å­¦ç¿’çµæžœä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = "trained_models/config_d"
        os.makedirs(output_dir, exist_ok=True)
        print(f"âœ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {output_dir}")
        
        # 6. å­¦ç¿’å®Ÿè¡Œ
        print("6. å­¦ç¿’é–‹å§‹...")
        
        # å­¦ç¿’é€²æ—ã®ç›£è¦–ç”¨å¤‰æ•°
        best_exploration_rate = 0.0
        exploration_rates = []
        episode_rewards_history = []
        system_rewards_history = []
        
        for episode in range(sim_param.episodeNum):
            if episode % 50 == 0:  # 50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«ãƒ­ã‚°
                print(f"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1}/{sim_param.episodeNum}")
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é–‹å§‹
            env.start_episode(episode)
            state = env.reset()
            
            episode_rewards = {swarm_id: 0.0 for swarm_id in swarm_agents.keys()}
            system_reward = 0.0
            
            # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
            for step in range(sim_param.maxStepsPerEpisode):
                # SystemAgentã®è¡Œå‹•å–å¾—
                system_observation = env.get_system_agent_observation()
                system_action = system_agent.get_action(system_observation)
                
                # SwarmAgentã®è¡Œå‹•å–å¾—
                swarm_actions = {}
                for swarm_id, agent in swarm_agents.items():
                    swarm_observation = env.get_swarm_agent_observation(swarm_id)
                    action_result = agent.get_action(swarm_observation)
                    
                    # get_actionã¯è¾žæ›¸ã‚’è¿”ã™ã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
                    if isinstance(action_result, dict):
                        swarm_actions[swarm_id] = action_result
                    else:
                        # ã‚¿ãƒ—ãƒ«ã®å ´åˆã¯æœ€åˆã®è¦ç´ ã‚’ä½¿ç”¨
                        swarm_actions[swarm_id] = action_result[0] if isinstance(action_result, tuple) else action_result
                
                # ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
                next_state, rewards, done, truncated, info = env.step(swarm_actions)
                
                # å ±é…¬ã®è“„ç©
                if isinstance(rewards, dict):
                    for swarm_id, reward in rewards.items():
                        if swarm_id in episode_rewards:
                            episode_rewards[swarm_id] += reward
                else:
                    # rewardsãŒè¾žæ›¸ã§ãªã„å ´åˆï¼ˆã‚¿ãƒ—ãƒ«ãªã©ï¼‰ã¯ã€æœ€åˆã®swarm_idã«å ±é…¬ã‚’è¿½åŠ 
                    if swarm_agents:
                        first_swarm_id = list(swarm_agents.keys())[0]
                        if first_swarm_id in episode_rewards:
                            episode_rewards[first_swarm_id] += rewards if isinstance(rewards, (int, float)) else 0.0
                
                # SystemAgentã®å ±é…¬ï¼ˆåˆ†å²ãƒ»çµ±åˆã®æˆåŠŸåº¦ã«åŸºã¥ãï¼‰
                system_reward += info.get('system_reward', 0.0)
                
                # æŽ¢æŸ»çŽ‡ç¢ºèª
                exploration_rate = env.get_exploration_rate()
                
                # ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
                if exploration_rate >= 0.8:
                    break
                
                if done or truncated:
                    break
            
            # å­¦ç¿’é€²æ—ã®è¨˜éŒ²
            exploration_rates.append(exploration_rate)
            episode_rewards_history.append(np.mean(list(episode_rewards.values())))
            system_rewards_history.append(system_reward)
            
            # ãƒ™ã‚¹ãƒˆè¨˜éŒ²ã®æ›´æ–°
            if exploration_rate > best_exploration_rate:
                best_exploration_rate = exploration_rate
            
            # é€²æ—è¡¨ç¤ºï¼ˆ50ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰
            if episode % 50 == 0:
                avg_reward = np.mean(list(episode_rewards.values()))
                avg_exploration = np.mean(exploration_rates[-50:]) if len(exploration_rates) >= 50 else np.mean(exploration_rates)
                avg_system_reward = np.mean(system_rewards_history[-50:]) if len(system_rewards_history) >= 50 else np.mean(system_rewards_history)
                print(f"    å¹³å‡å ±é…¬: {avg_reward:.3f}, å¹³å‡æŽ¢æŸ»çŽ‡: {avg_exploration:.3f}, ãƒ™ã‚¹ãƒˆæŽ¢æŸ»çŽ‡: {best_exploration_rate:.3f}, å¹³å‡ã‚·ã‚¹ãƒ†ãƒ å ±é…¬: {avg_system_reward:.3f}")
            
            # å­¦ç¿’ã®æ—©æœŸçµ‚äº†æ¡ä»¶ï¼ˆ500ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ä»¥é™ã§æŽ¢æŸ»çŽ‡ãŒå®‰å®šã—ãŸå ´åˆï¼‰
            if episode >= 500 and len(exploration_rates) >= 100:
                recent_avg = np.mean(exploration_rates[-100:])
                if recent_avg > 0.7 and abs(recent_avg - np.mean(exploration_rates[-200:-100])) < 0.02:
                    print(f"    å­¦ç¿’ãŒåŽæŸã—ã¾ã—ãŸã€‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {episode + 1} ã§çµ‚äº†")
                    break
        
        # 7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        print("7. ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
        
        # SystemAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜
        system_model_path = os.path.join(output_dir, "system_agent_model.h5")
        system_agent.save_model(system_model_path)
        print(f"  âœ“ SystemAgent ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {system_model_path}")
        
        # SwarmAgentãƒ¢ãƒ‡ãƒ«ä¿å­˜
        for swarm_id, agent in swarm_agents.items():
            model_path = os.path.join(output_dir, f"swarm_agent_model_{swarm_id}.h5")
            agent.save_model(model_path)
            print(f"  âœ“ SwarmAgent {swarm_id} ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")
        
        print("âœ“ å­¦ç¿’å®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("=== Config_D å­¦ç¿’é–‹å§‹ ===")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now()}")
    
    success = run_training()
    
    print(f"\nçµ‚äº†æ™‚åˆ»: {datetime.now()}")
    if success:
        print("ðŸŽ‰ å­¦ç¿’ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    else:
        print("âŒ å­¦ç¿’ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        sys.exit(1) 