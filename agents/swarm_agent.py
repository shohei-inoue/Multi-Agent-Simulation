"""
ç¾¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹
ç§»å‹•æ–¹å‘ã®æ±ºå®šã®ã¿ã‚’è¡Œã„ã€åˆ†å²ãƒ»çµ±åˆã®åˆ¤å®šã¯è¡Œã‚ãªã„
åˆ†å²æ™‚ã«ã¯æ–°ã—ã„ç¾¤ã‚’ç”Ÿæˆã€çµ±åˆæ™‚ã«ã¯ä»–ã®ç¾¤ã«å–ã‚Šè¾¼ã¾ã‚Œã‚‹
"""

from agents.base_agent import BaseAgent
import tensorflow as tf
import numpy as np
from typing import Dict, Any, Tuple, Optional
from utils.utils import flatten_state
from params.swarm_agent import SwarmAgentParam


class SwarmAgent(BaseAgent):
    """
    ç¾¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - ç§»å‹•æ–¹å‘ã®æ±ºå®šã®ã¿ã‚’è¡Œã†
    åˆ†å²ãƒ»çµ±åˆã®åˆ¤å®šãƒ»ãƒˆãƒªã‚¬ãƒ¼ã¯è¡Œã‚ãªã„
    """
    def __init__(self, env, algorithm, model, action_space, param: SwarmAgentParam, system_agent=None, swarm_id=None):
        super().__init__(env, algorithm, model, action_space=action_space)
        self.action_space = action_space
        self.param = param
        self.isLearning = param.isLearning
        self.learningParameter = param.learningParameter
        self.debug = param.debug
        self.system_agent = system_agent
        self.swarm_id = swarm_id

    def get_action(self, state: Dict[str, Any], episode: int = 0, log_dir: Optional[str] = None) -> Tuple[tf.Tensor, Dict[str, Any]]:
        """
        ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–å¾—ï¼ˆå­¦ç¿’ãƒ­ã‚°æ©Ÿèƒ½ä»˜ãï¼‰
        Args:
            state: ç¾åœ¨ã®çŠ¶æ…‹
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            log_dir: ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        Returns:
            action: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆthetaï¼‰
            action_info: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±
        """
        # å­¦ç¿’ãªã—ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if not self.isLearning or self.model is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å®Ÿè¡Œ
            default_params = np.array([0.5, 10.0, 5.0])  # th, k_e, k_c
            theta, valid_directions = self.algorithm.policy(state, default_params)
            
            # åˆ†å²ãƒ»çµ±åˆæ¡ä»¶ã®åˆ¤å®š
            follower_scores = state.get("follower_mobility_scores", [])
            follower_count = len(follower_scores)
            avg_mobility = np.mean(follower_scores) if follower_count > 0 else 0.0
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è©³ç´°ã«å‡ºåŠ›
            print(f"ðŸ” SwarmAgent {self.swarm_id}ï¼ˆå­¦ç¿’ãªã—ï¼‰: follower_scoresè©³ç´°åˆ†æž")
            print(f"   - follower_scores: {follower_scores}")
            print(f"   - follower_count: {follower_count}")
            print(f"   - avg_mobility: {avg_mobility:.6f}")
            print(f"   - å„ã‚¹ã‚³ã‚¢ã®è©³ç´°:")
            for i, score in enumerate(follower_scores):
                print(f"     [{i}]: {score:.6f}")
            
            print(f"ðŸ“Š SwarmAgent {self.swarm_id}ï¼ˆå­¦ç¿’ãªã—ï¼‰: follower_scores={follower_scores}, follower_count={follower_count}, avg_mobility={avg_mobility:.3f}, valid_directions={len(valid_directions)}")
            
            # SystemAgentã‹ã‚‰é–¾å€¤ã‚’å–å¾—
            branch_threshold = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            integration_threshold = 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            if self.system_agent and hasattr(self.system_agent, 'model'):
                if hasattr(self.system_agent.model, 'get_branch_threshold'):
                    branch_threshold = self.system_agent.model.get_branch_threshold() or 0.5
                if hasattr(self.system_agent.model, 'get_integration_threshold'):
                    integration_threshold = self.system_agent.model.get_integration_threshold() or 0.3
            
            should_branch = (
                follower_count >= 3 and
                valid_directions and len(valid_directions) >= 2 and
                avg_mobility >= branch_threshold
            )
            should_integrate = avg_mobility < integration_threshold
            
            # system_agentã«é€ã‚‹stateã‚’ç”Ÿæˆ
            system_state = {
                "theta": theta,
                "valid_directions": valid_directions,
                "swarm_id": self.swarm_id,
                "follower_count": follower_count,
                "swarm_count": state.get("swarm_count", 1),
                "swarm_mobility_score": follower_scores,
                "avg_mobility": avg_mobility
            }
            
            # åˆ†å²åˆ¤å®š
            if self.system_agent and should_branch:
                print(f"ðŸ”¥ SwarmAgent {self.swarm_id}: åˆ†å²æ¡ä»¶æº€ãŸã— - check_branchå‘¼ã³å‡ºã—")
                self.system_agent.check_branch(system_state)
            elif self.system_agent:
                print(f"ðŸ” SwarmAgent {self.swarm_id}: åˆ†å²æ¡ä»¶ãƒã‚§ãƒƒã‚¯ - follower_count={follower_count}(è¦æ±‚â‰¥3), valid_directions={len(valid_directions)}(è¦æ±‚â‰¥2), avg_mobility={avg_mobility:.3f}(è¦æ±‚â‰¥{branch_threshold})")
            
            # çµ±åˆåˆ¤å®š
            if self.system_agent and should_integrate:
                print(f"ðŸ”¥ SwarmAgent {self.swarm_id}: çµ±åˆæ¡ä»¶æº€ãŸã— - check_integrationå‘¼ã³å‡ºã—")
                self.system_agent.check_integration(system_state)
            elif self.system_agent:
                print(f"ðŸ” SwarmAgent {self.swarm_id}: çµ±åˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯ - avg_mobility={avg_mobility:.3f}(è¦æ±‚<{integration_threshold})")
            
            return {"theta": theta}, {
                'theta': theta,
                'valid_directions': valid_directions
            }
        
        # å­¦ç¿’ã‚ã‚Šãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        assert self.model is not None, "model must not be None"
        
        # çŠ¶æ…‹ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        state_vec = tf.convert_to_tensor([flatten_state(state)], dtype=tf.float32)
        
        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        learning_mu, learning_std, theta_mu, theta_std, value = self.model(state_vec)
        
        # å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        learning_params = self.model.sample_learning_params(learning_mu, learning_std)
        
        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦ãƒãƒªã‚·ãƒ¼ã‚’å®Ÿè¡Œ
        theta, valid_directions = self.algorithm.policy(state, learning_params)
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆthetaï¼‰ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        action_theta = self.model.sample_action(theta_mu, theta_std)
        
        # å­¦ç¿’ãƒ­ã‚°ã‚’è¨˜éŒ²
        if log_dir and hasattr(self, 'logger'):
            self._log_learning_metrics(episode, {
                'learning_th': float(learning_params[0]),
                'learning_k_e': float(learning_params[1]),
                'learning_k_c': float(learning_params[2]),
                'action_theta': float(action_theta),
                'value': float(value),
                'valid_directions_count': len(valid_directions)
            }, log_dir)
        
        # thetaãŒnumpy.float64ã®å ´åˆã¯floatã«å¤‰æ›
        theta_value = float(theta) if hasattr(theta, 'numpy') else theta
        
        action = {"theta": theta_value, "valid_directions": valid_directions}
        
        if self.debug and self.debug.enable_debug_log:
            print(f"SwarmAgent action | theta: {theta_value:.3f} ({np.rad2deg(theta_value):.1f}[deg])")
            print(f"SwarmAgent params | th: {learning_params[0]:.3f}, k_e: {learning_params[1]:.3f}, k_c: {learning_params[2]:.3f}")
            print(f"SwarmAgent valid_directions: {len(valid_directions)}")
        
        # åˆ†å²ãƒ»çµ±åˆæ¡ä»¶ã®åˆ¤å®š
        follower_scores = state.get("follower_mobility_scores", [])
        follower_count = len(follower_scores)
        avg_mobility = np.mean(follower_scores) if follower_count > 0 else 0.0
        
        # SystemAgentã‹ã‚‰é–¾å€¤ã‚’å–å¾—
        branch_threshold = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        integration_threshold = 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        if self.system_agent and hasattr(self.system_agent, 'model'):
            if hasattr(self.system_agent.model, 'get_branch_threshold'):
                branch_threshold = self.system_agent.model.get_branch_threshold() or 0.5
            if hasattr(self.system_agent.model, 'get_integration_threshold'):
                integration_threshold = self.system_agent.model.get_integration_threshold() or 0.3
        
        should_branch = (
            follower_count >= 3 and
            valid_directions and len(valid_directions) >= 2 and
            avg_mobility >= branch_threshold
        )
        should_integrate = avg_mobility < integration_threshold
        
        # system_agentã«é€ã‚‹stateã‚’ç”Ÿæˆ
        system_state = {
            "theta": theta_value,
            "valid_directions": valid_directions,
            "swarm_id": self.swarm_id,
            "follower_count": follower_count,
            "swarm_count": state.get("swarm_count", 1),
            "swarm_mobility_score": follower_scores,
            "avg_mobility": avg_mobility
        }
        
        # åˆ†å²åˆ¤å®š
        if self.system_agent and should_branch:
            print(f"ðŸ”¥ SwarmAgent {self.swarm_id}: åˆ†å²æ¡ä»¶æº€ãŸã— - check_branchå‘¼ã³å‡ºã—ï¼ˆå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            self.system_agent.check_branch(system_state)
        elif self.system_agent:
            print(f"ðŸ” SwarmAgent {self.swarm_id}: åˆ†å²æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰ - follower_count={follower_count}(è¦æ±‚â‰¥3), valid_directions={len(valid_directions)}(è¦æ±‚â‰¥2), avg_mobility={avg_mobility:.3f}(è¦æ±‚â‰¥{branch_threshold})")
        
        # çµ±åˆåˆ¤å®š
        if self.system_agent and should_integrate:
            print(f"ðŸ”¥ SwarmAgent {self.swarm_id}: çµ±åˆæ¡ä»¶æº€ãŸã— - check_integrationå‘¼ã³å‡ºã—ï¼ˆå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            self.system_agent.check_integration(system_state)
        elif self.system_agent:
            print(f"ðŸ” SwarmAgent {self.swarm_id}: çµ±åˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆå­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰ - avg_mobility={avg_mobility:.3f}(è¦æ±‚<{integration_threshold})")
        
        return {"theta": theta_value}, {
            'theta': float(action_theta) if not isinstance(action_theta, tuple) else float(action_theta[0]),
            'learning_params': learning_params.numpy().tolist() if hasattr(learning_params, 'numpy') else list(learning_params),
            'valid_directions': valid_directions,
            'value': float(value)
        }

    def _log_learning_metrics(self, episode: int, metrics: Dict[str, float], log_dir: str):
        """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°ã«è¨˜éŒ²"""
        try:
            from utils.logger import create_experiment_logger
            logger = create_experiment_logger(log_dir, "swarm_learning")
            logger.log_learning_progress(episode, "swarm", metrics)
            logger.close()
        except ImportError:
            # ãƒ­ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç„¡è¦–
            pass

    def train(self, *args, **kwargs):
        """
        å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã¯å¿…è¦ã«å¿œã˜ã¦å®Ÿè£…
        """
        pass 