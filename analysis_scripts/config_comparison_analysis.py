#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Config Comparison Analysis Script
Config A, B, C, Dã®å·®ã‚’è©³ç´°ã«åˆ†æã™ã‚‹

Configè¨­å®š:
- Config A: VFH-Fuzzy ã®ã¿ï¼ˆåˆ†å²ãƒ»çµ±åˆãªã—ã€å­¦ç¿’ãªã—ï¼‰
- Config B: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼ˆåˆ†å²ãƒ»çµ±åˆãªã—ï¼‰
- Config C: åˆ†å²ãƒ»çµ±åˆã‚ã‚Šï¼ˆå­¦ç¿’ãªã—ï¼‰
- Config D: åˆ†å²ãƒ»çµ±åˆã‚ã‚Š + å­¦ç¿’ã‚ã‚Š

ä½¿ç”¨æ–¹æ³•:
    python config_comparison_analysis.py

å‡ºåŠ›:
    - config_comparison_results/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«çµæœã‚’ä¿å­˜
    - PNGå½¢å¼ã®ã‚°ãƒ©ãƒ•
    - CSVå½¢å¼ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Any
from scipy import stats
from scipy.stats import ttest_ind
import itertools

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.style.use('default')

class ConfigComparisonAnalyzer:
    def __init__(self, data_dir: str = "verify_configs/verification_results"):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir: æ¤œè¨¼çµæœãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.data_dir = Path(data_dir)
        self.episode_data = pd.DataFrame()
        self.step_data = pd.DataFrame()
        
        # Configè¨­å®šã®èª¬æ˜
        self.config_descriptions = {
            'A': 'VFH-Fuzzy only (No branching/integration, No learning)',
            'B': 'Pre-trained model (No branching/integration)',
            'C': 'Branching/Integration enabled (No learning)',
            'D': 'Branching/Integration + Learning'
        }
        
        # Configè‰²è¨­å®š
        self.config_colors = {
            'A': '#3498db',  # é’
            'B': '#e74c3c',  # èµ¤
            'C': '#2ecc71',  # ç·‘
            'D': '#f39c12'   # ã‚ªãƒ¬ãƒ³ã‚¸
        }
        
    def load_all_data(self) -> bool:
        """
        å…¨ã¦ã®Configã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            bool: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸå¯å¦
        """
        print("=== Configæ¯”è¼ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ ===")
        
        if not self.data_dir.exists():
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_dir}")
            return False
        
        episode_records = []
        step_records = []
        
        # å„Configãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢
        for config_dir in self.data_dir.iterdir():
            if not config_dir.is_dir():
                continue
                
            config_name = config_dir.name
            print(f"  ğŸ“‚ {config_name} ã‚’å‡¦ç†ä¸­...")
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            json_files = list(config_dir.glob("*.json"))
            if not json_files:
                print(f"    âš ï¸  JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    episodes_data = None
                    if isinstance(data, dict) and 'episodes' in data:
                        episodes_data = data['episodes']
                    elif isinstance(data, list):
                        episodes_data = data
                    
                    if episodes_data:
                        for episode in episodes_data:
                            # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
                            episode_info = {
                                'config_type': config_name.split('_')[1].upper(),  # A, B, C, D
                                'obstacle_density': float(config_name.split('_')[3]),
                                'episode_id': episode.get('episode', 1),
                                'final_exploration_rate': episode.get('final_exploration_rate', 0.0),
                                'steps_taken': episode.get('steps_taken', 0),
                                'steps_to_target': episode.get('steps_to_target', None),
                                'total_reward': episode.get('total_reward', 0.0),
                                'avg_reward': episode.get('avg_reward', 0.0),
                                'file_source': json_file.name
                            }
                            episode_records.append(episode_info)
                            
                            # ã‚¹ãƒ†ãƒƒãƒ—è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                            if 'step_details' in episode:
                                for step_detail in episode['step_details']:
                                    step_info = {
                                        'config_type': config_name.split('_')[1].upper(),
                                        'obstacle_density': float(config_name.split('_')[3]),
                                        'episode_id': episode.get('episode', 1),
                                        'step_id': step_detail.get('step', 0),
                                        'exploration_rate': step_detail.get('exploration_rate', 0.0),
                                        'reward': step_detail.get('reward', 0.0),
                                        'swarm_count': step_detail.get('swarm_count', 1),
                                        'agent_collision_flag': step_detail.get('agent_collision_flag', 0),
                                        'follower_collision_count': step_detail.get('follower_collision_count', 0),
                                        'file_source': json_file.name
                                    }
                                    step_records.append(step_info)
                    
                    print(f"    âœ“ {json_file.name} å‡¦ç†å®Œäº†")
                    
                except Exception as e:
                    print(f"    âŒ {json_file.name} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # DataFrameã«å¤‰æ›
        if episode_records:
            self.episode_data = pd.DataFrame(episode_records)
            print(f"âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿: {len(self.episode_data)} ä»¶")
        else:
            print("âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
        if step_records:
            self.step_data = pd.DataFrame(step_records)
            print(f"âœ“ ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿: {len(self.step_data)} ä»¶")
        else:
            print("âŒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        return len(episode_records) > 0
    
    def generate_config_overview(self, output_dir: str = "config_comparison_results"):
        """
        Configæ¦‚è¦æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== Configæ¦‚è¦æ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if self.episode_data.empty:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return
        
        plt.figure(figsize=(18, 12))
        
        # 1. Configåˆ¥å¹³å‡æ¢æŸ»ç‡
        plt.subplot(2, 4, 1)
        config_exploration = self.episode_data.groupby('config_type')['final_exploration_rate'].mean()
        config_exploration_std = self.episode_data.groupby('config_type')['final_exploration_rate'].std()
        
        bars = plt.bar(config_exploration.index, config_exploration.values, 
                      color=[self.config_colors[c] for c in config_exploration.index],
                      alpha=0.8, capsize=5)
        plt.errorbar(config_exploration.index, config_exploration.values, 
                    yerr=config_exploration_std.values, fmt='none', color='black', capsize=5)
        
        plt.title('Average Final Exploration Rate by Config')
        plt.xlabel('Config Type')
        plt.ylabel('Final Exploration Rate')
        plt.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, val in zip(bars, config_exploration.values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{val:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. Configåˆ¥å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ•°
        plt.subplot(2, 4, 2)
        config_steps = self.episode_data.groupby('config_type')['steps_taken'].mean()
        config_steps_std = self.episode_data.groupby('config_type')['steps_taken'].std()
        
        bars = plt.bar(config_steps.index, config_steps.values,
                      color=[self.config_colors[c] for c in config_steps.index],
                      alpha=0.8, capsize=5)
        plt.errorbar(config_steps.index, config_steps.values,
                    yerr=config_steps_std.values, fmt='none', color='black', capsize=5)
        
        plt.title('Average Steps Taken by Config')
        plt.xlabel('Config Type')
        plt.ylabel('Steps Taken')
        plt.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, val in zip(bars, config_steps.values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                    f'{val:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. æ¢æŸ»åŠ¹ç‡ï¼ˆæ¢æŸ»ç‡/ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        plt.subplot(2, 4, 3)
        self.episode_data['exploration_efficiency'] = self.episode_data['final_exploration_rate'] / self.episode_data['steps_taken']
        config_efficiency = self.episode_data.groupby('config_type')['exploration_efficiency'].mean()
        config_efficiency_std = self.episode_data.groupby('config_type')['exploration_efficiency'].std()
        
        bars = plt.bar(config_efficiency.index, config_efficiency.values,
                      color=[self.config_colors[c] for c in config_efficiency.index],
                      alpha=0.8, capsize=5)
        plt.errorbar(config_efficiency.index, config_efficiency.values,
                    yerr=config_efficiency_std.values, fmt='none', color='black', capsize=5)
        
        plt.title('Exploration Efficiency by Config')
        plt.xlabel('Config Type')
        plt.ylabel('Exploration Rate / Steps')
        plt.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, val in zip(bars, config_efficiency.values):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0001, 
                    f'{val:.4f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. Configåˆ¥æ¢æŸ»ç‡åˆ†å¸ƒï¼ˆãƒã‚¤ã‚ªãƒªãƒ³ãƒ—ãƒ­ãƒƒãƒˆï¼‰
        plt.subplot(2, 4, 4)
        config_types = sorted(self.episode_data['config_type'].unique())
        exploration_data = [
            self.episode_data[self.episode_data['config_type'] == config]['final_exploration_rate'].values
            for config in config_types
        ]
        
        parts = plt.violinplot(exploration_data, positions=range(len(config_types)), showmeans=True)
        for i, (part, config) in enumerate(zip(parts['bodies'], config_types)):
            part.set_facecolor(self.config_colors[config])
            part.set_alpha(0.7)
        
        plt.xticks(range(len(config_types)), [f'Config {c}' for c in config_types])
        plt.title('Exploration Rate Distribution by Config')
        plt.ylabel('Final Exploration Rate')
        plt.grid(True, alpha=0.3)
        
        # 5. éšœå®³ç‰©å¯†åº¦ã®å½±éŸ¿
        plt.subplot(2, 4, 5)
        for config_type in sorted(self.episode_data['config_type'].unique()):
            config_data = self.episode_data[self.episode_data['config_type'] == config_type]
            density_means = config_data.groupby('obstacle_density')['final_exploration_rate'].mean()
            plt.plot(density_means.index, density_means.values, 
                    color=self.config_colors[config_type], marker='o', linewidth=2, markersize=8,
                    label=f'Config {config_type}')
        
        plt.title('Impact of Obstacle Density by Config')
        plt.xlabel('Obstacle Density')
        plt.ylabel('Average Exploration Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é€²æ—ï¼ˆConfigåˆ¥ï¼‰
        plt.subplot(2, 4, 6)
        for config_type in sorted(self.episode_data['config_type'].unique()):
            config_data = self.episode_data[self.episode_data['config_type'] == config_type]
            episode_means = config_data.groupby('episode_id')['final_exploration_rate'].mean()
            plt.plot(episode_means.index, episode_means.values,
                    color=self.config_colors[config_type], marker='s', linewidth=2, markersize=6,
                    label=f'Config {config_type}')
        
        plt.title('Learning Progress by Config')
        plt.xlabel('Episode')
        plt.ylabel('Average Exploration Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 7. æ¢æŸ»ç‡ vs ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®é–¢ä¿‚
        plt.subplot(2, 4, 7)
        for config_type in sorted(self.episode_data['config_type'].unique()):
            config_data = self.episode_data[self.episode_data['config_type'] == config_type]
            plt.scatter(config_data['steps_taken'], config_data['final_exploration_rate'],
                       c=self.config_colors[config_type], alpha=0.6, s=50,
                       label=f'Config {config_type}')
        
        plt.xlabel('Steps Taken')
        plt.ylabel('Final Exploration Rate')
        plt.title('Exploration Rate vs Steps by Config')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 8. Configèª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ
        plt.subplot(2, 4, 8)
        plt.axis('off')
        y_pos = 0.9
        for config, desc in self.config_descriptions.items():
            plt.text(0.05, y_pos, f'Config {config}:', fontweight='bold', fontsize=12,
                    color=self.config_colors[config], transform=plt.gca().transAxes)
            plt.text(0.05, y_pos-0.05, desc, fontsize=10, wrap=True,
                    transform=plt.gca().transAxes)
            y_pos -= 0.2
        
        plt.title('Configuration Descriptions')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/config_overview_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ Configæ¦‚è¦æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_step_analysis(self, output_dir: str = "config_comparison_results"):
        """
        ã‚¹ãƒ†ãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã®è©³ç´°åˆ†æ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== ã‚¹ãƒ†ãƒƒãƒ—ãƒ¬ãƒ™ãƒ«è©³ç´°åˆ†æä¸­ ===")
        
        if self.step_data.empty:
            print("âŒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        plt.figure(figsize=(16, 12))
        
        # 1. ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ¢æŸ»ç‡å¤‰åŒ–ï¼ˆå…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å¹³å‡ï¼‰
        plt.subplot(2, 3, 1)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            avg_exploration_by_step = config_steps.groupby('step_id')['exploration_rate'].mean()
            plt.plot(avg_exploration_by_step.index, avg_exploration_by_step.values,
                    color=self.config_colors[config_type], linewidth=2, marker='o', markersize=4,
                    label=f'Config {config_type}')
        
        plt.title('Exploration Rate Progress by Config')
        plt.xlabel('Step')
        plt.ylabel('Average Exploration Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ã‚¹ãƒ¯ãƒ¼ãƒ æ•°ã®å¤‰åŒ–
        plt.subplot(2, 3, 2)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            avg_swarms_by_step = config_steps.groupby('step_id')['swarm_count'].mean()
            plt.plot(avg_swarms_by_step.index, avg_swarms_by_step.values,
                    color=self.config_colors[config_type], linewidth=2, marker='^', markersize=4,
                    label=f'Config {config_type}')
        
        plt.title('Swarm Count Progress by Config')
        plt.xlabel('Step')
        plt.ylabel('Average Swarm Count')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. è¡çªç‡ã®å¤‰åŒ–
        plt.subplot(2, 3, 3)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            collision_rate_by_step = config_steps.groupby('step_id')['agent_collision_flag'].mean()
            plt.plot(collision_rate_by_step.index, collision_rate_by_step.values,
                    color=self.config_colors[config_type], linewidth=2, marker='x', markersize=4,
                    label=f'Config {config_type}')
        
        plt.title('Collision Rate Progress by Config')
        plt.xlabel('Step')
        plt.ylabel('Collision Rate')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. å ±é…¬ã®å¤‰åŒ–
        plt.subplot(2, 3, 4)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            avg_reward_by_step = config_steps.groupby('step_id')['reward'].mean()
            plt.plot(avg_reward_by_step.index, avg_reward_by_step.values,
                    color=self.config_colors[config_type], linewidth=2, marker='s', markersize=4,
                    label=f'Config {config_type}')
        
        plt.title('Reward Progress by Config')
        plt.xlabel('Step')
        plt.ylabel('Average Reward')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. æ¢æŸ»åŠ¹ç‡ã®å¤‰åŒ–
        plt.subplot(2, 3, 5)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            efficiency_data = []
            step_ids = []
            for step_id in sorted(config_steps['step_id'].unique()):
                step_data = config_steps[config_steps['step_id'] == step_id]
                if step_id > 0:
                    efficiency = step_data['exploration_rate'].mean() / step_id
                    efficiency_data.append(efficiency)
                    step_ids.append(step_id)
            
            if efficiency_data:
                plt.plot(step_ids, efficiency_data,
                        color=self.config_colors[config_type], linewidth=2, marker='d', markersize=4,
                        label=f'Config {config_type}')
        
        plt.title('Exploration Efficiency Progress by Config')
        plt.xlabel('Step')
        plt.ylabel('Exploration Rate / Step')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 6. ç´¯ç©å ±é…¬
        plt.subplot(2, 3, 6)
        for config_type in sorted(self.step_data['config_type'].unique()):
            config_steps = self.step_data[self.step_data['config_type'] == config_type]
            cumulative_reward = config_steps.groupby('step_id')['reward'].sum().cumsum()
            plt.plot(cumulative_reward.index, cumulative_reward.values,
                    color=self.config_colors[config_type], linewidth=2,
                    label=f'Config {config_type}')
        
        plt.title('Cumulative Reward by Config')
        plt.xlabel('Step')
        plt.ylabel('Cumulative Reward')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/config_step_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ ã‚¹ãƒ†ãƒƒãƒ—ãƒ¬ãƒ™ãƒ«åˆ†æã‚°ãƒ©ãƒ•ã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_statistical_comparison(self, output_dir: str = "config_comparison_results"):
        """
        çµ±è¨ˆçš„æ¯”è¼ƒåˆ†æ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== çµ±è¨ˆçš„æ¯”è¼ƒåˆ†æå®Ÿè¡Œä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if self.episode_data.empty:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # åŸºæœ¬çµ±è¨ˆé‡
        stats_summary = []
        config_types = sorted(self.episode_data['config_type'].unique())
        
        for config_type in config_types:
            config_data = self.episode_data[self.episode_data['config_type'] == config_type]
            
            stats_row = {
                'Config': config_type,
                'Description': self.config_descriptions[config_type],
                'Sample_Count': len(config_data),
                'Exploration_Rate_Mean': config_data['final_exploration_rate'].mean(),
                'Exploration_Rate_Std': config_data['final_exploration_rate'].std(),
                'Exploration_Rate_Min': config_data['final_exploration_rate'].min(),
                'Exploration_Rate_Max': config_data['final_exploration_rate'].max(),
                'Steps_Mean': config_data['steps_taken'].mean(),
                'Steps_Std': config_data['steps_taken'].std(),
                'Steps_Min': config_data['steps_taken'].min(),
                'Steps_Max': config_data['steps_taken'].max(),
                'Exploration_Efficiency_Mean': (config_data['final_exploration_rate'] / config_data['steps_taken']).mean(),
                'Exploration_Efficiency_Std': (config_data['final_exploration_rate'] / config_data['steps_taken']).std(),
            }
            
            if 'total_reward' in config_data.columns:
                stats_row.update({
                    'Total_Reward_Mean': config_data['total_reward'].mean(),
                    'Total_Reward_Std': config_data['total_reward'].std(),
                })
            
            stats_summary.append(stats_row)
        
        # çµ±è¨ˆçµæœã‚’CSVã§ä¿å­˜
        stats_df = pd.DataFrame(stats_summary)
        stats_df.to_csv(f"{output_dir}/config_comparison_statistics.csv", index=False)
        
        # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºtæ¤œå®šï¼ˆæ¢æŸ»ç‡ï¼‰
        t_test_results = []
        
        for config1, config2 in itertools.combinations(config_types, 2):
            data1 = self.episode_data[self.episode_data['config_type'] == config1]['final_exploration_rate']
            data2 = self.episode_data[self.episode_data['config_type'] == config2]['final_exploration_rate']
            
            if len(data1) > 1 and len(data2) > 1:
                t_stat, p_value = ttest_ind(data1, data2)
                
                t_test_results.append({
                    'Config_1': config1,
                    'Config_2': config2,
                    'Mean_1': data1.mean(),
                    'Mean_2': data2.mean(),
                    'Mean_Diff': data1.mean() - data2.mean(),
                    'T_statistic': t_stat,
                    'P_value': p_value,
                    'Significant': p_value < 0.05,
                    'Effect_Size': abs(data1.mean() - data2.mean()) / np.sqrt((data1.var() + data2.var()) / 2)
                })
        
        # tæ¤œå®šçµæœã‚’CSVã§ä¿å­˜
        if t_test_results:
            t_test_df = pd.DataFrame(t_test_results)
            t_test_df.to_csv(f"{output_dir}/config_pairwise_ttest.csv", index=False)
            
            print("âœ“ ãƒšã‚¢ãƒ¯ã‚¤ã‚ºtæ¤œå®šçµæœ:")
            for result in t_test_results:
                significance = "æœ‰æ„" if result['Significant'] else "éæœ‰æ„"
                print(f"  Config {result['Config_1']} vs {result['Config_2']}: "
                      f"p={result['P_value']:.4f} ({significance}), "
                      f"åŠ¹æœã‚µã‚¤ã‚º={result['Effect_Size']:.4f}")
        
        # ANOVAåˆ†æ
        config_groups = []
        for config_type in config_types:
            config_data = self.episode_data[
                self.episode_data['config_type'] == config_type
            ]['final_exploration_rate'].values
            
            if len(config_data) > 0:
                config_groups.append(config_data)
        
        if len(config_groups) > 1:
            try:
                f_stat, p_value = stats.f_oneway(*config_groups)
                anova_result = {
                    'F_statistic': f_stat,
                    'p_value': p_value,
                    'significant': p_value < 0.05,
                    'configs_tested': config_types
                }
                
                # ANOVAçµæœã‚’ä¿å­˜
                with open(f"{output_dir}/config_anova_result.json", 'w') as f:
                    json.dump(anova_result, f, indent=2)
                
                print(f"âœ“ ANOVAåˆ†æçµæœ: F={f_stat:.4f}, p={p_value:.4f}")
                
            except Exception as e:
                print(f"âŒ ANOVAåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ“ çµ±è¨ˆåˆ†æçµæœã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_performance_ranking(self, output_dir: str = "config_comparison_results"):
        """
        æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°åˆ†æå®Ÿè¡Œä¸­ ===")
        
        # å„æŒ‡æ¨™ã§ã®é †ä½ã‚’è¨ˆç®—
        config_performance = self.episode_data.groupby('config_type').agg({
            'final_exploration_rate': ['mean', 'std'],
            'steps_taken': ['mean', 'std'],
        }).round(4)
        
        # æ¢æŸ»åŠ¹ç‡ã‚’è¿½åŠ 
        config_performance[('exploration_efficiency', 'mean')] = (
            self.episode_data.groupby('config_type')['final_exploration_rate'].mean() /
            self.episode_data.groupby('config_type')['steps_taken'].mean()
        )
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ã‚’ä½œæˆ
        ranking_data = []
        
        # æ¢æŸ»ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        exploration_ranking = config_performance[('final_exploration_rate', 'mean')].sort_values(ascending=False)
        for rank, (config, value) in enumerate(exploration_ranking.items(), 1):
            ranking_data.append({
                'Metric': 'Final Exploration Rate',
                'Rank': rank,
                'Config': config,
                'Value': f"{value:.4f}",
                'Description': self.config_descriptions[config]
            })
        
        # æ¢æŸ»åŠ¹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        efficiency_ranking = config_performance[('exploration_efficiency', 'mean')].sort_values(ascending=False)
        for rank, (config, value) in enumerate(efficiency_ranking.items(), 1):
            ranking_data.append({
                'Metric': 'Exploration Efficiency',
                'Rank': rank,
                'Config': config,
                'Value': f"{value:.6f}",
                'Description': self.config_descriptions[config]
            })
        
        # ã‚¹ãƒ†ãƒƒãƒ—æ•°ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå°‘ãªã„æ–¹ãŒè‰¯ã„ï¼‰
        steps_ranking = config_performance[('steps_taken', 'mean')].sort_values(ascending=True)
        for rank, (config, value) in enumerate(steps_ranking.items(), 1):
            ranking_data.append({
                'Metric': 'Steps Taken (Lower is Better)',
                'Rank': rank,
                'Config': config,
                'Value': f"{value:.1f}",
                'Description': self.config_descriptions[config]
            })
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’CSVã§ä¿å­˜
        ranking_df = pd.DataFrame(ranking_data)
        ranking_df.to_csv(f"{output_dir}/config_performance_ranking.csv", index=False)
        
        print(f"âœ“ æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœã‚’è¡¨ç¤º
        print("\nğŸ“Š æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°çµæœ:")
        for metric in ['Final Exploration Rate', 'Exploration Efficiency', 'Steps Taken (Lower is Better)']:
            print(f"\n{metric}:")
            metric_data = ranking_df[ranking_df['Metric'] == metric].sort_values('Rank')
            for _, row in metric_data.iterrows():
                print(f"  {row['Rank']}ä½: Config {row['Config']} - {row['Value']}")
    
    def generate_summary_report(self, output_dir: str = "config_comparison_results"):
        """
        ç·åˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== ç·åˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if self.episode_data.empty:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        report_lines = []
        report_lines.append("# Config Comparison Analysis Report")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        # ãƒ‡ãƒ¼ã‚¿æ¦‚è¦
        report_lines.append("## Data Overview")
        report_lines.append(f"- Total configurations analyzed: {len(self.episode_data['config_type'].unique())}")
        report_lines.append(f"- Total episodes: {len(self.episode_data)}")
        report_lines.append(f"- Obstacle densities: {sorted(self.episode_data['obstacle_density'].unique())}")
        report_lines.append("")
        
        # Configèª¬æ˜
        report_lines.append("## Configuration Descriptions")
        for config, desc in self.config_descriptions.items():
            report_lines.append(f"- **Config {config}**: {desc}")
        report_lines.append("")
        
        # æ€§èƒ½ã‚µãƒãƒªãƒ¼
        report_lines.append("## Performance Summary")
        config_summary = self.episode_data.groupby('config_type').agg({
            'final_exploration_rate': ['mean', 'std', 'count'],
            'steps_taken': ['mean', 'std'],
        }).round(4)
        
        for config in sorted(self.episode_data['config_type'].unique()):
            exploration_mean = config_summary.loc[config, ('final_exploration_rate', 'mean')]
            exploration_std = config_summary.loc[config, ('final_exploration_rate', 'std')]
            steps_mean = config_summary.loc[config, ('steps_taken', 'mean')]
            count = config_summary.loc[config, ('final_exploration_rate', 'count')]
            
            report_lines.append(f"### Config {config}")
            report_lines.append(f"- Episodes analyzed: {count}")
            report_lines.append(f"- Average exploration rate: {exploration_mean:.4f} Â± {exploration_std:.4f}")
            report_lines.append(f"- Average steps taken: {steps_mean:.1f}")
            report_lines.append(f"- Exploration efficiency: {exploration_mean/steps_mean:.6f}")
            report_lines.append("")
        
        # ä¸»è¦ãªç™ºè¦‹
        report_lines.append("## Key Findings")
        
        # æœ€é«˜æ€§èƒ½ã®Config
        best_exploration = self.episode_data.groupby('config_type')['final_exploration_rate'].mean()
        best_config = best_exploration.idxmax()
        best_value = best_exploration.max()
        
        report_lines.append(f"- **Highest exploration rate**: Config {best_config} ({best_value:.4f})")
        
        # æœ€é«˜åŠ¹ç‡ã®Config
        efficiency = (self.episode_data.groupby('config_type')['final_exploration_rate'].mean() /
                     self.episode_data.groupby('config_type')['steps_taken'].mean())
        most_efficient = efficiency.idxmax()
        efficiency_value = efficiency.max()
        
        report_lines.append(f"- **Most efficient**: Config {most_efficient} ({efficiency_value:.6f} exploration/step)")
        
        # æœ€å°‘ã‚¹ãƒ†ãƒƒãƒ—ã®Config
        min_steps = self.episode_data.groupby('config_type')['steps_taken'].mean()
        fastest_config = min_steps.idxmin()
        fastest_value = min_steps.min()
        
        report_lines.append(f"- **Fastest completion**: Config {fastest_config} ({fastest_value:.1f} steps)")
        
        report_lines.append("")
        
        # çµ±è¨ˆçš„æœ‰æ„å·®
        report_lines.append("## Statistical Significance")
        report_lines.append("Based on pairwise t-tests (p < 0.05):")
        
        # tæ¤œå®šçµæœã‚’èª­ã¿è¾¼ã¿ï¼ˆæ—¢ã«è¨ˆç®—æ¸ˆã¿ã®å ´åˆï¼‰
        t_test_file = Path(f"{output_dir}/config_pairwise_ttest.csv")
        if t_test_file.exists():
            t_test_df = pd.read_csv(t_test_file)
            significant_pairs = t_test_df[t_test_df['Significant'] == True]
            
            if len(significant_pairs) > 0:
                for _, row in significant_pairs.iterrows():
                    report_lines.append(f"- Config {row['Config_1']} vs {row['Config_2']}: "
                                      f"p={row['P_value']:.4f}, effect size={row['Effect_Size']:.4f}")
            else:
                report_lines.append("- No statistically significant differences found")
        
        report_lines.append("")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(f"{output_dir}/config_comparison_report.md", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ“ ç·åˆã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def run_complete_analysis(self):
        """
        å®Œå…¨ãªConfigæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œ
        """
        print("ğŸš€ Configæ¯”è¼ƒåˆ†æã‚’é–‹å§‹ã—ã¾ã™\n")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_all_data():
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        output_dir = "config_comparison_results"
        
        # åˆ†æå®Ÿè¡Œ
        self.generate_config_overview(output_dir)
        self.generate_step_analysis(output_dir)
        self.generate_statistical_comparison(output_dir)
        self.generate_performance_ranking(output_dir)
        self.generate_summary_report(output_dir)
        
        print(f"\nğŸ‰ Configæ¯”è¼ƒåˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ çµæœã¯ {output_dir}/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = ConfigComparisonAnalyzer()
    analyzer.run_complete_analysis()


if __name__ == "__main__":
    main() 