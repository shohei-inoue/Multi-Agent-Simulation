#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Best Performance Comparison Script
å„Configã§æœ€çµ‚æ¢æŸ»ç‡ãŒæœ€ã‚‚è‰¯ã‹ã£ãŸç’°å¢ƒã‚’æŠ½å‡ºã—ã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ¢æŸ»ç‡ä¸Šæ˜‡ã‚’æ¯”è¼ƒ

ä½¿ç”¨æ–¹æ³•:
    python best_performance_comparison.py

å‡ºåŠ›:
    - best_performance_comparison/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«çµæœã‚’ä¿å­˜
    - å„Configã®æœ€é«˜æ€§èƒ½æ™‚ã®æ¢æŸ»ç‡ä¸Šæ˜‡æ¯”è¼ƒã‚°ãƒ©ãƒ•
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Any, Tuple

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.style.use('default')

class BestPerformanceAnalyzer:
    def __init__(self, data_dir: str = "verify_configs/verification_results"):
        """
        åˆæœŸåŒ–
        
        Args:
            data_dir: æ¤œè¨¼çµæœãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.data_dir = Path(data_dir)
        self.step_data = pd.DataFrame()
        self.episode_data = pd.DataFrame()
        self.best_configs = {}  # å„Configã®æœ€é«˜æ€§èƒ½ç’°å¢ƒã‚’æ ¼ç´
        
        # Configè¨­å®š
        self.config_descriptions = {
            'A': 'VFH-Fuzzy only',
            'B': 'Pre-trained model',
            'C': 'Branching/Integration',
            'D': 'Branching/Integration + Learning'
        }
        
        self.config_colors = {
            'A': '#3498db',  # é’
            'B': '#e74c3c',  # èµ¤
            'C': '#2ecc71',  # ç·‘
            'D': '#f39c12'   # ã‚ªãƒ¬ãƒ³ã‚¸
        }
        
    def load_first_episode_data(self) -> bool:
        """
        1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç›®ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            bool: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸå¯å¦
        """
        print("=== 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç›®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­ ===")
        
        if not self.data_dir.exists():
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_dir}")
            return False
        
        episode_records = []
        step_records = []
        
        # å„Configãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ç´¢
        for config_dir in self.data_dir.iterdir():
            if not config_dir.is_dir():
                continue
                
            config_name = config_dir.name
            print(f"  ğŸ“‚ {config_name} ã‚’å‡¦ç†ä¸­...")
            
            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
            json_files = list(config_dir.glob("*.json"))
            if not json_files:
                print(f"    âš ï¸  JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue
            
            for json_file in json_files:
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # 1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç›®ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
                    episodes_data = None
                    if isinstance(data, dict) and 'episodes' in data:
                        episodes_data = data['episodes']
                    elif isinstance(data, list):
                        episodes_data = data
                    
                    if episodes_data and len(episodes_data) > 0:
                        first_episode = episodes_data[0]  # æœ€åˆã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
                        
                        config_type = config_name.split('_')[1].upper()
                        obstacle_density = float(config_name.split('_')[3])
                        
                        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
                        episode_info = {
                            'config_type': config_type,
                            'obstacle_density': obstacle_density,
                            'episode_id': first_episode.get('episode', 1),
                            'final_exploration_rate': first_episode.get('final_exploration_rate', 0.0),
                            'steps_taken': first_episode.get('steps_taken', 0),
                            'steps_to_target': first_episode.get('steps_to_target', None),
                            'total_reward': first_episode.get('total_reward', 0.0),
                            'avg_reward': first_episode.get('avg_reward', 0.0),
                            'file_source': json_file.name
                        }
                        episode_records.append(episode_info)
                        
                        # ã‚¹ãƒ†ãƒƒãƒ—è©³ç´°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                        if 'step_details' in first_episode:
                            for step_detail in first_episode['step_details']:
                                step_info = {
                                    'config_type': config_type,
                                    'obstacle_density': obstacle_density,
                                    'episode_id': first_episode.get('episode', 1),
                                    'step_id': step_detail.get('step', 0),
                                    'exploration_rate': step_detail.get('exploration_rate', 0.0),
                                    'reward': step_detail.get('reward', 0.0),
                                    'swarm_count': step_detail.get('swarm_count', 1),
                                    'agent_collision_flag': step_detail.get('agent_collision_flag', 0),
                                    'follower_collision_count': step_detail.get('follower_collision_count', 0),
                                    'file_source': json_file.name
                                }
                                step_records.append(step_info)
                    
                    print(f"    âœ“ {json_file.name} å‡¦ç†å®Œäº†")
                    
                except Exception as e:
                    print(f"    âŒ {json_file.name} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # DataFrameã«å¤‰æ›
        if episode_records:
            self.episode_data = pd.DataFrame(episode_records)
            print(f"âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿: {len(self.episode_data)} ä»¶")
        else:
            print("âŒ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
        if step_records:
            self.step_data = pd.DataFrame(step_records)
            print(f"âœ“ ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿: {len(self.step_data)} ä»¶")
        else:
            print("âŒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        return len(episode_records) > 0
    
    def identify_best_configs(self) -> Dict[str, Tuple[float, float]]:
        """
        å„Configã§æœ€çµ‚æ¢æŸ»ç‡ãŒæœ€ã‚‚è‰¯ã‹ã£ãŸç’°å¢ƒï¼ˆéšœå®³ç‰©å¯†åº¦ï¼‰ã‚’ç‰¹å®š
        
        Returns:
            Dict[str, Tuple[float, float]]: {config_type: (obstacle_density, final_exploration_rate)}
        """
        print("=== å„Configæœ€é«˜æ€§èƒ½ç’°å¢ƒã®ç‰¹å®šä¸­ ===")
        
        best_configs = {}
        
        for config_type in sorted(self.episode_data['config_type'].unique()):
            config_data = self.episode_data[self.episode_data['config_type'] == config_type]
            
            # æœ€é«˜ã®æœ€çµ‚æ¢æŸ»ç‡ã‚’æŒã¤ç’°å¢ƒã‚’ç‰¹å®š
            best_row = config_data.loc[config_data['final_exploration_rate'].idxmax()]
            
            best_density = best_row['obstacle_density']
            best_exploration_rate = best_row['final_exploration_rate']
            
            best_configs[config_type] = (best_density, best_exploration_rate)
            
            print(f"  Config {config_type}: éšœå®³ç‰©å¯†åº¦ {best_density} ã§æœ€é«˜æ¢æŸ»ç‡ {best_exploration_rate:.4f}")
        
        self.best_configs = best_configs
        return best_configs
    
    def generate_best_performance_comparison(self, output_dir: str = "best_performance_comparison"):
        """
        å„Configã®æœ€é«˜æ€§èƒ½æ™‚ã®æ¢æŸ»ç‡ä¸Šæ˜‡ã‚’æ¯”è¼ƒ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== æœ€é«˜æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        
        if self.step_data.empty or not self.best_configs:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return
        
        # å¤§ããªã‚µã‚¤ã‚ºã§ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
        plt.figure(figsize=(16, 12))
        
        # ãƒ¡ã‚¤ãƒ³ã®æ¯”è¼ƒã‚°ãƒ©ãƒ•
        plt.subplot(2, 2, 1)
        
        best_performance_data = []
        
        for config_type, (best_density, best_exploration_rate) in self.best_configs.items():
            # è©²å½“ã™ã‚‹Configã¨ç’°å¢ƒã®ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            config_step_data = self.step_data[
                (self.step_data['config_type'] == config_type) & 
                (self.step_data['obstacle_density'] == best_density)
            ]
            
            if len(config_step_data) > 0:
                # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å¹³å‡æ¢æŸ»ç‡ã‚’è¨ˆç®—
                step_stats = config_step_data.groupby('step_id')['exploration_rate'].agg(['mean', 'std']).reset_index()
                
                # ãƒ¡ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³
                plt.plot(step_stats['step_id'], step_stats['mean'],
                        color=self.config_colors[config_type], 
                        linewidth=4, 
                        marker='o', 
                        markersize=8,
                        label=f'Config {config_type}: {self.config_descriptions[config_type]}\n(Density: {best_density}, Final: {best_exploration_rate:.4f})',
                        alpha=0.9)
                
                # æ¨™æº–åå·®ã‚’å½±ã§è¡¨ç¤º
                plt.fill_between(step_stats['step_id'],
                               step_stats['mean'] - step_stats['std'],
                               step_stats['mean'] + step_stats['std'],
                               color=self.config_colors[config_type], 
                               alpha=0.2)
                
                # åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                best_performance_data.append({
                    'config_type': config_type,
                    'best_density': best_density,
                    'final_exploration_rate': best_exploration_rate,
                    'step_data': step_stats
                })
        
        plt.title('Best Performance Comparison - Exploration Rate Progress', 
                 fontsize=16, fontweight='bold', pad=15)
        plt.xlabel('Step', fontsize=14, fontweight='bold')
        plt.ylabel('Exploration Rate', fontsize=14, fontweight='bold')
        plt.legend(loc='upper left', fontsize=10, frameon=True, fancybox=True, shadow=True)
        plt.grid(True, alpha=0.4)
        plt.xlim(left=0)
        plt.ylim(bottom=0)
        
        # æ¢æŸ»ç‡ä¸Šæ˜‡é€Ÿåº¦ã®æ¯”è¼ƒ
        plt.subplot(2, 2, 2)
        
        for data in best_performance_data:
            step_means = data['step_data']['mean'].values
            if len(step_means) > 1:
                # æ¢æŸ»ç‡ã®å¤‰åŒ–ç‡ã‚’è¨ˆç®—ï¼ˆå¾®åˆ†ã®è¿‘ä¼¼ï¼‰
                exploration_diff = np.diff(step_means)
                step_ids = data['step_data']['step_id'].values[1:]
                
                plt.plot(step_ids, exploration_diff,
                        color=self.config_colors[data['config_type']], 
                        linewidth=3, 
                        marker='s', 
                        markersize=6,
                        label=f'Config {data["config_type"]}',
                        alpha=0.8)
        
        plt.title('Exploration Rate Increase Speed Comparison', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Step', fontsize=12, fontweight='bold')
        plt.ylabel('Exploration Rate Increase per Step', fontsize=12, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.4)
        
        # ç´¯ç©æ¢æŸ»ç‡ã®æ¯”è¼ƒ
        plt.subplot(2, 2, 3)
        
        for data in best_performance_data:
            step_stats = data['step_data']
            plt.plot(step_stats['step_id'], step_stats['mean'],
                    color=self.config_colors[data['config_type']], 
                    linewidth=3,
                    label=f'Config {data["config_type"]} (Final: {data["final_exploration_rate"]:.4f})',
                    alpha=0.8)
        
        plt.title('Cumulative Exploration Rate Comparison', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Step', fontsize=12, fontweight='bold')
        plt.ylabel('Exploration Rate', fontsize=12, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(True, alpha=0.4)
        
        # æœ€çµ‚æ¢æŸ»ç‡ã®ãƒãƒ¼æ¯”è¼ƒ
        plt.subplot(2, 2, 4)
        
        configs = list(self.best_configs.keys())
        final_rates = [self.best_configs[config][1] for config in configs]
        densities = [self.best_configs[config][0] for config in configs]
        
        bars = plt.bar(configs, final_rates, 
                      color=[self.config_colors[config] for config in configs],
                      alpha=0.8, capsize=5)
        
        # ãƒãƒ¼ã®ä¸Šã«æ•°å€¤ã¨ç’°å¢ƒæƒ…å ±ã‚’è¡¨ç¤º
        for bar, rate, density in zip(bars, final_rates, densities):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{rate:.4f}\n(Density: {density})', 
                    ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        plt.title('Final Exploration Rate Comparison\n(Best Performance Environment)', 
                 fontsize=14, fontweight='bold')
        plt.xlabel('Config Type', fontsize=12, fontweight='bold')
        plt.ylabel('Final Exploration Rate', fontsize=12, fontweight='bold')
        plt.grid(True, alpha=0.4, axis='y')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/best_performance_comparison.png", 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        
        # è©³ç´°ãªå˜ä½“ã‚°ãƒ©ãƒ•ã‚‚ç”Ÿæˆ
        self.generate_detailed_single_graph(output_dir, best_performance_data)
        
        print(f"âœ“ æœ€é«˜æ€§èƒ½æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_detailed_single_graph(self, output_dir: str, best_performance_data: List[Dict]):
        """
        è©³ç´°ãªå˜ä½“ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            best_performance_data: æœ€é«˜æ€§èƒ½ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        # å¤§ããªå˜ä½“ã‚°ãƒ©ãƒ•
        plt.figure(figsize=(16, 10))
        
        for data in best_performance_data:
            step_stats = data['step_data']
            config_type = data['config_type']
            
            plt.plot(step_stats['step_id'], step_stats['mean'],
                    color=self.config_colors[config_type], 
                    linewidth=4, 
                    marker='o', 
                    markersize=8,
                    label=f'Config {config_type}: {self.config_descriptions[config_type]}\n'
                          f'Best Environment - Density: {data["best_density"]}, Final Rate: {data["final_exploration_rate"]:.4f}',
                    alpha=0.9)
            
            # æ¨™æº–åå·®ã‚’å½±ã§è¡¨ç¤º
            plt.fill_between(step_stats['step_id'],
                           step_stats['mean'] - step_stats['std'],
                           step_stats['mean'] + step_stats['std'],
                           color=self.config_colors[config_type], 
                           alpha=0.2)
        
        plt.title('Best Performance Exploration Rate Progress Comparison\nEach Config at Their Optimal Environment', 
                 fontsize=18, fontweight='bold', pad=20)
        plt.xlabel('Step', fontsize=16, fontweight='bold')
        plt.ylabel('Exploration Rate', fontsize=16, fontweight='bold')
        
        plt.legend(loc='upper left', fontsize=12, frameon=True, fancybox=True, shadow=True, framealpha=0.9)
        plt.grid(True, alpha=0.4, linestyle='-', linewidth=0.5)
        plt.grid(True, alpha=0.2, linestyle='--', linewidth=0.3, which='minor')
        
        plt.xlim(left=0)
        plt.ylim(bottom=0)
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        
        # èƒŒæ™¯è‰²ã‚’è¨­å®š
        plt.gca().set_facecolor('#f8f9fa')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/best_performance_detailed.png", 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
    
    def generate_summary_statistics(self, output_dir: str = "best_performance_comparison"):
        """
        æœ€é«˜æ€§èƒ½ã®çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        print("=== æœ€é«˜æ€§èƒ½çµ±è¨ˆã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­ ===")
        
        summary_data = []
        
        for config_type, (best_density, best_exploration_rate) in self.best_configs.items():
            # è©²å½“ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            config_step_data = self.step_data[
                (self.step_data['config_type'] == config_type) & 
                (self.step_data['obstacle_density'] == best_density)
            ]
            
            if len(config_step_data) > 0:
                step_means = config_step_data.groupby('step_id')['exploration_rate'].mean()
                
                # æ¢æŸ»ç‡ä¸Šæ˜‡é€Ÿåº¦ã‚’è¨ˆç®—
                if len(step_means) > 1:
                    exploration_diff = np.diff(step_means.values)
                    avg_increase_rate = np.mean(exploration_diff)
                    max_increase_rate = np.max(exploration_diff)
                    min_increase_rate = np.min(exploration_diff)
                else:
                    avg_increase_rate = max_increase_rate = min_increase_rate = 0
                
                # ç›®æ¨™æ¢æŸ»ç‡åˆ°é”æ™‚é–“ã‚’è¨ˆç®—
                target_rates = [0.1, 0.2, 0.3, 0.4, 0.5]
                achievement_times = {}
                
                for target_rate in target_rates:
                    achieved_steps = step_means[step_means >= target_rate]
                    if len(achieved_steps) > 0:
                        achievement_times[f'time_to_{target_rate}'] = achieved_steps.index[0]
                    else:
                        achievement_times[f'time_to_{target_rate}'] = None
                
                summary_row = {
                    'config_type': config_type,
                    'config_description': self.config_descriptions[config_type],
                    'best_obstacle_density': best_density,
                    'final_exploration_rate': best_exploration_rate,
                    'avg_increase_rate': avg_increase_rate,
                    'max_increase_rate': max_increase_rate,
                    'min_increase_rate': min_increase_rate,
                    'total_steps': len(step_means),
                    'exploration_efficiency': best_exploration_rate / len(step_means) if len(step_means) > 0 else 0,
                    **achievement_times
                }
                
                summary_data.append(summary_row)
        
        # CSVã§ä¿å­˜
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(f"{output_dir}/best_performance_statistics.csv", index=False)
            print(f"âœ“ æœ€é«˜æ€§èƒ½çµ±è¨ˆã‚’ {output_dir}/best_performance_statistics.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # æ¯”è¼ƒè¡¨ã‚‚ç”Ÿæˆ
        self.generate_comparison_table(output_dir, summary_data)
    
    def generate_comparison_table(self, output_dir: str, summary_data: List[Dict]):
        """
        æ¯”è¼ƒè¡¨ã‚’ç”Ÿæˆ
        
        Args:
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            summary_data: çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
        """
        comparison_data = []
        
        for data in summary_data:
            comparison_row = {
                'Config': data['config_type'],
                'Description': data['config_description'],
                'Best_Environment': f"Density {data['best_obstacle_density']}",
                'Final_Exploration_Rate': f"{data['final_exploration_rate']:.4f}",
                'Avg_Increase_Rate': f"{data['avg_increase_rate']:.6f}",
                'Max_Increase_Rate': f"{data['max_increase_rate']:.6f}",
                'Exploration_Efficiency': f"{data['exploration_efficiency']:.6f}",
                'Time_to_30%': data.get('time_to_0.3', 'N/A')
            }
            comparison_data.append(comparison_row)
        
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            comparison_df.to_csv(f"{output_dir}/best_performance_comparison_table.csv", index=False)
            print(f"âœ“ æ¯”è¼ƒè¡¨ã‚’ {output_dir}/best_performance_comparison_table.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def run_analysis(self):
        """
        åˆ†æã‚’å®Ÿè¡Œ
        """
        print("ğŸš€ å„Configæœ€é«˜æ€§èƒ½æ¯”è¼ƒåˆ†æã‚’é–‹å§‹ã—ã¾ã™\n")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.load_first_episode_data():
            print("âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        # æœ€é«˜æ€§èƒ½ç’°å¢ƒã‚’ç‰¹å®š
        best_configs = self.identify_best_configs()
        if not best_configs:
            print("âŒ æœ€é«˜æ€§èƒ½ç’°å¢ƒã®ç‰¹å®šã«å¤±æ•—ã—ã¾ã—ãŸ")
            return False
        
        output_dir = "best_performance_comparison"
        
        # åˆ†æå®Ÿè¡Œ
        self.generate_best_performance_comparison(output_dir)
        self.generate_summary_statistics(output_dir)
        
        print(f"\nğŸ‰ å„Configæœ€é«˜æ€§èƒ½æ¯”è¼ƒåˆ†æå®Œäº†ï¼")
        print(f"ğŸ“ çµæœã¯ {output_dir}/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        
        # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\nğŸ“Š æœ€é«˜æ€§èƒ½ã‚µãƒãƒªãƒ¼:")
        for config_type, (density, rate) in best_configs.items():
            print(f"  Config {config_type}: éšœå®³ç‰©å¯†åº¦ {density} ã§æœ€çµ‚æ¢æŸ»ç‡ {rate:.4f}")
        
        return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = BestPerformanceAnalyzer()
    analyzer.run_analysis()


if __name__ == "__main__":
    main() 