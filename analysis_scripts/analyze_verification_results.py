#!/usr/bin/env python3
"""
ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœè§£æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
verify_configs/verification_resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœã‚’è§£æã—ã€
å„Configã®æ€§èƒ½ã‚’æ¯”è¼ƒãƒ»å¯è¦–åŒ–ã—ã¾ã™ã€‚
"""

import os
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

class VerificationResultAnalyzer:
    """ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœè§£æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, results_dir: str = "verify_configs/verification_results"):
        """
        åˆæœŸåŒ–
        Args:
            results_dir: çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        """
        self.results_dir = Path(results_dir)
        self.results_data = {}
        self.summary_stats = {}
        
    def load_results(self):
        """ã™ã¹ã¦ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print("=== ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœèª­ã¿è¾¼ã¿ä¸­ ===")
        
        for config_dir in self.results_dir.iterdir():
            if config_dir.is_dir():
                config_name = config_dir.name
                json_file = config_dir / "verification_result.json"
                
                if json_file.exists():
                    print(f"èª­ã¿è¾¼ã¿ä¸­: {config_name}")
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        self.results_data[config_name] = data
                        print(f"  âœ“ {config_name}: {len(data.get('episodes', []))} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
                    except Exception as e:
                        print(f"  âŒ {config_name}: ã‚¨ãƒ©ãƒ¼ - {e}")
                else:
                    print(f"  âš ï¸ {config_name}: JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        print(f"âœ“ åˆè¨ˆ {len(self.results_data)} å€‹ã®çµæœã‚’èª­ã¿è¾¼ã¿å®Œäº†\n")
    
    def extract_config_info(self, config_name: str) -> Tuple[str, float]:
        """Configåã‹ã‚‰è¨­å®šæƒ…å ±ã‚’æŠ½å‡º"""
        parts = config_name.replace('Config_', '').split('_obstacle_')
        config_type = parts[0]  # A, B, C, D
        obstacle_density = float(parts[1]) if len(parts) > 1 else 0.0
        return config_type, obstacle_density
    
    def calculate_summary_statistics(self):
        """å„Configã®çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
        print("=== çµ±è¨ˆæƒ…å ±è¨ˆç®—ä¸­ ===")
        
        for config_name, data in self.results_data.items():
            config_type, obstacle_density = self.extract_config_info(config_name)
            episodes = data.get('episodes', [])
            
            if not episodes:
                continue
            
            # åŸºæœ¬çµ±è¨ˆ
            exploration_rates = [ep.get('final_exploration_rate', 0) for ep in episodes]
            steps_taken = [ep.get('steps_taken', 0) for ep in episodes]
            
            # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°çµ±è¨ˆ
            episode_stats = []
            for ep in episodes:
                step_details = ep.get('step_details', [])
                if step_details:
                    rewards = [step.get('reward', 0) for step in step_details]
                    episode_stats.append({
                        'total_reward': sum(rewards),
                        'avg_reward': np.mean(rewards),
                        'max_reward': max(rewards),
                        'min_reward': min(rewards)
                    })
            
            # çµ±è¨ˆã‚µãƒãƒªãƒ¼
            stats = {
                'config_type': config_type,
                'obstacle_density': obstacle_density,
                'total_episodes': len(episodes),
                'exploration_rate': {
                    'mean': np.mean(exploration_rates),
                    'std': np.std(exploration_rates),
                    'min': np.min(exploration_rates),
                    'max': np.max(exploration_rates),
                    'median': np.median(exploration_rates)
                },
                'steps_taken': {
                    'mean': np.mean(steps_taken),
                    'std': np.std(steps_taken),
                    'min': np.min(steps_taken),
                    'max': np.max(steps_taken),
                    'median': np.median(steps_taken)
                }
            }
            
            if episode_stats:
                total_rewards = [es['total_reward'] for es in episode_stats]
                avg_rewards = [es['avg_reward'] for es in episode_stats]
                
                stats['total_reward'] = {
                    'mean': np.mean(total_rewards),
                    'std': np.std(total_rewards),
                    'min': np.min(total_rewards),
                    'max': np.max(total_rewards),
                    'median': np.median(total_rewards)
                }
                
                stats['avg_reward'] = {
                    'mean': np.mean(avg_rewards),
                    'std': np.std(avg_rewards),
                    'min': np.min(avg_rewards),
                    'max': np.max(avg_rewards),
                    'median': np.median(avg_rewards)
                }
            
            self.summary_stats[config_name] = stats
            print(f"  âœ“ {config_name}: æ¢æŸ»ç‡å¹³å‡ {stats['exploration_rate']['mean']:.3f}")
        
        print("âœ“ çµ±è¨ˆæƒ…å ±è¨ˆç®—å®Œäº†\n")
    
    def create_comparison_dataframe(self) -> pd.DataFrame:
        """æ¯”è¼ƒç”¨ã®DataFrameã‚’ä½œæˆ"""
        rows = []
        
        for config_name, stats in self.summary_stats.items():
            row = {
                'Config': config_name,
                'ConfigType': stats['config_type'],
                'ObstacleDensity': stats['obstacle_density'],
                'Episodes': stats['total_episodes'],
                'ExplorationRate_Mean': stats['exploration_rate']['mean'],
                'ExplorationRate_Std': stats['exploration_rate']['std'],
                'ExplorationRate_Min': stats['exploration_rate']['min'],
                'ExplorationRate_Max': stats['exploration_rate']['max'],
                'StepsTaken_Mean': stats['steps_taken']['mean'],
                'StepsTaken_Std': stats['steps_taken']['std']
            }
            
            if 'total_reward' in stats:
                row.update({
                    'TotalReward_Mean': stats['total_reward']['mean'],
                    'TotalReward_Std': stats['total_reward']['std'],
                    'AvgReward_Mean': stats['avg_reward']['mean'],
                    'AvgReward_Std': stats['avg_reward']['std']
                })
            
            rows.append(row)
        
        return pd.DataFrame(rows)
    
    def generate_visualizations(self, output_dir: str = "analysis_results"):
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ"""
        print("=== å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        df = self.create_comparison_dataframe()
        
        if df.empty:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã€å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return
        
        # 1. Exploration Rate Comparison (by Config)
        plt.figure(figsize=(15, 10))
        
        # 1-1. Average Exploration Rate by Config
        plt.subplot(2, 3, 1)
        config_means = df.groupby('ConfigType')['ExplorationRate_Mean'].mean()
        config_means.plot(kind='bar', color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
        plt.title('Average Exploration Rate by Config')
        plt.xlabel('Config Type')
        plt.ylabel('Exploration Rate')
        plt.xticks(rotation=0)
        
        # 1-2. Average Exploration Rate by Obstacle Density
        plt.subplot(2, 3, 2)
        density_means = df.groupby('ObstacleDensity')['ExplorationRate_Mean'].mean()
        density_means.plot(kind='bar', color=['lightblue', 'orange', 'lightcoral'])
        plt.title('Average Exploration Rate by Obstacle Density')
        plt.xlabel('Obstacle Density')
        plt.ylabel('Exploration Rate')
        plt.xticks(rotation=0)
        
        # 1-3. ConfigÃ—éšœå®³ç‰©å¯†åº¦ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        plt.subplot(2, 3, 3)
        pivot_data = df.pivot_table(values='ExplorationRate_Mean', 
                                   index='ConfigType', 
                                   columns='ObstacleDensity', 
                                   aggfunc='mean')
        sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlOrRd')
        plt.title('Exploration Rate Heatmap')
        
        # 1-4. Exploration Rate Distribution (Box Plot)
        plt.subplot(2, 3, 4)
        config_types = df['ConfigType'].unique()
        exploration_data = []
        labels = []
        
        for config_type in sorted(config_types):
            config_data = df[df['ConfigType'] == config_type]['ExplorationRate_Mean'].values
            exploration_data.append(config_data)
            labels.append(f'Config {config_type}')
        
        plt.boxplot(exploration_data, labels=labels)
        plt.title('Exploration Rate Distribution by Config')
        plt.ylabel('Exploration Rate')
        plt.xticks(rotation=45)
        
        # 1-5. Steps Comparison
        plt.subplot(2, 3, 5)
        step_means = df.groupby('ConfigType')['StepsTaken_Mean'].mean()
        step_means.plot(kind='bar', color=['lightsteelblue', 'lightpink', 'lightseagreen', 'lightyellow'])
        plt.title('Average Steps by Config')
        plt.xlabel('Config Type')
        plt.ylabel('Steps')
        plt.xticks(rotation=0)
        
        # 1-6. Exploration Rate vs Steps Scatter Plot
        plt.subplot(2, 3, 6)
        colors = {'A': 'blue', 'B': 'red', 'C': 'green', 'D': 'orange'}
        for config_type in df['ConfigType'].unique():
            config_df = df[df['ConfigType'] == config_type]
            plt.scatter(config_df['StepsTaken_Mean'], 
                       config_df['ExplorationRate_Mean'],
                       c=colors.get(config_type, 'gray'),
                       label=f'Config {config_type}',
                       alpha=0.7, s=100)
        
        plt.xlabel('Average Steps')
        plt.ylabel('Average Exploration Rate')
        plt.title('Exploration Rate vs Steps')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/exploration_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. Detailed Comparison Table
        if 'TotalReward_Mean' in df.columns:
            plt.figure(figsize=(12, 8))
            
            # 2-1. Reward Comparison
            plt.subplot(2, 2, 1)
            reward_means = df.groupby('ConfigType')['TotalReward_Mean'].mean()
            reward_means.plot(kind='bar', color=['lightcyan', 'mistyrose', 'honeydew', 'lemonchiffon'])
            plt.title('Average Total Reward by Config')
            plt.xlabel('Config Type')
            plt.ylabel('Total Reward')
            plt.xticks(rotation=0)
            
            # 2-2. Reward Efficiency (Reward/Step)
            plt.subplot(2, 2, 2)
            df['RewardEfficiency'] = df['TotalReward_Mean'] / df['StepsTaken_Mean']
            efficiency_means = df.groupby('ConfigType')['RewardEfficiency'].mean()
            efficiency_means.plot(kind='bar', color=['lightblue', 'lightpink', 'lightgreen', 'lightyellow'])
            plt.title('Reward Efficiency by Config')
            plt.xlabel('Config Type')
            plt.ylabel('Reward/Step')
            plt.xticks(rotation=0)
            
            # 2-3. Exploration Rate vs Total Reward Scatter Plot
            plt.subplot(2, 2, 3)
            for config_type in df['ConfigType'].unique():
                config_df = df[df['ConfigType'] == config_type]
                plt.scatter(config_df['TotalReward_Mean'], 
                           config_df['ExplorationRate_Mean'],
                           c=colors.get(config_type, 'gray'),
                           label=f'Config {config_type}',
                           alpha=0.7, s=100)
            
            plt.xlabel('Average Total Reward')
            plt.ylabel('Average Exploration Rate')
            plt.title('Exploration Rate vs Total Reward')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # 2-4. Impact of Obstacle Density
            plt.subplot(2, 2, 4)
            for density in sorted(df['ObstacleDensity'].unique()):
                density_df = df[df['ObstacleDensity'] == density]
                plt.plot(density_df['ConfigType'], 
                        density_df['ExplorationRate_Mean'], 
                        marker='o', label=f'Density {density}', linewidth=2, markersize=8)
            
            plt.xlabel('Config Type')
            plt.ylabel('Average Exploration Rate')
            plt.title('Impact of Obstacle Density')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f"{output_dir}/reward_analysis.png", dpi=300, bbox_inches='tight')
            plt.close()
        
        print(f"âœ“ å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ {output_dir}/ ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def generate_summary_report(self, output_dir: str = "analysis_results"):
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("=== ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ ===")
        
        os.makedirs(output_dir, exist_ok=True)
        df = self.create_comparison_dataframe()
        
        report_path = f"{output_dir}/verification_summary_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœè§£æãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # 1. æ¦‚è¦
            f.write("## 1. è§£ææ¦‚è¦\n\n")
            f.write(f"- è§£æå¯¾è±¡Configæ•°: {len(df)}\n")
            f.write(f"- Configç¨®é¡: {', '.join(sorted(df['ConfigType'].unique()))}\n")
            f.write(f"- éšœå®³ç‰©å¯†åº¦: {', '.join(map(str, sorted(df['ObstacleDensity'].unique())))}\n")
            f.write(f"- ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {df['Episodes'].sum()}\n\n")
            
            # 2. Configåˆ¥æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            f.write("## 2. Configåˆ¥æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n\n")
            
            # æ¢æŸ»ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            f.write("### 2.1 æ¢æŸ»ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé«˜ã„é †ï¼‰\n\n")
            exploration_ranking = df.groupby('ConfigType')['ExplorationRate_Mean'].mean().sort_values(ascending=False)
            for i, (config, rate) in enumerate(exploration_ranking.items(), 1):
                f.write(f"{i}. Config {config}: {rate:.4f}\n")
            f.write("\n")
            
            # åŠ¹ç‡æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ¢æŸ»ç‡/ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
            f.write("### 2.2 åŠ¹ç‡æ€§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆæ¢æŸ»ç‡/ã‚¹ãƒ†ãƒƒãƒ—ï¼‰\n\n")
            df['Efficiency'] = df['ExplorationRate_Mean'] / df['StepsTaken_Mean']
            efficiency_ranking = df.groupby('ConfigType')['Efficiency'].mean().sort_values(ascending=False)
            for i, (config, eff) in enumerate(efficiency_ranking.items(), 1):
                f.write(f"{i}. Config {config}: {eff:.6f}\n")
            f.write("\n")
            
            # 3. è©³ç´°çµ±è¨ˆè¡¨
            f.write("## 3. è©³ç´°çµ±è¨ˆè¡¨\n\n")
            f.write("| Config | éšœå®³ç‰©å¯†åº¦ | ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•° | æ¢æŸ»ç‡(å¹³å‡Â±æ¨™æº–åå·®) | ã‚¹ãƒ†ãƒƒãƒ—æ•°(å¹³å‡Â±æ¨™æº–åå·®) |\n")
            f.write("|--------|------------|--------------|----------------------|------------------------|\n")
            
            for _, row in df.iterrows():
                f.write(f"| {row['ConfigType']} | {row['ObstacleDensity']:.3f} | {row['Episodes']} | "
                       f"{row['ExplorationRate_Mean']:.4f}Â±{row['ExplorationRate_Std']:.4f} | "
                       f"{row['StepsTaken_Mean']:.1f}Â±{row['StepsTaken_Std']:.1f} |\n")
            f.write("\n")
            
            # 4. çµ±è¨ˆçš„åˆ†æ
            f.write("## 4. çµ±è¨ˆçš„åˆ†æ\n\n")
            
            # Configé–“ã®æ¯”è¼ƒ
            f.write("### 4.1 Configé–“ã®æ¢æŸ»ç‡æ¯”è¼ƒ\n\n")
            config_stats = df.groupby('ConfigType')['ExplorationRate_Mean'].agg(['mean', 'std', 'min', 'max'])
            for config in config_stats.index:
                stats = config_stats.loc[config]
                f.write(f"**Config {config}**\n")
                f.write(f"- å¹³å‡: {stats['mean']:.4f}\n")
                f.write(f"- æ¨™æº–åå·®: {stats['std']:.4f}\n")
                f.write(f"- ç¯„å›²: {stats['min']:.4f} - {stats['max']:.4f}\n\n")
            
            # éšœå®³ç‰©å¯†åº¦ã®å½±éŸ¿
            f.write("### 4.2 éšœå®³ç‰©å¯†åº¦ã®å½±éŸ¿\n\n")
            density_stats = df.groupby('ObstacleDensity')['ExplorationRate_Mean'].agg(['mean', 'std'])
            for density in density_stats.index:
                stats = density_stats.loc[density]
                f.write(f"**éšœå®³ç‰©å¯†åº¦ {density:.3f}**\n")
                f.write(f"- å¹³å‡æ¢æŸ»ç‡: {stats['mean']:.4f}Â±{stats['std']:.4f}\n\n")
            
            # 5. æ¨å¥¨äº‹é …
            f.write("## 5. æ¨å¥¨äº‹é …\n\n")
            
            best_config = exploration_ranking.index[0]
            best_rate = exploration_ranking.iloc[0]
            f.write(f"- **æœ€é«˜æ€§èƒ½**: Config {best_config} (æ¢æŸ»ç‡: {best_rate:.4f})\n")
            
            most_efficient = efficiency_ranking.index[0]
            best_efficiency = efficiency_ranking.iloc[0]
            f.write(f"- **æœ€é«˜åŠ¹ç‡**: Config {most_efficient} (åŠ¹ç‡æ€§: {best_efficiency:.6f})\n")
            
            # éšœå®³ç‰©å¯†åº¦ã®å½±éŸ¿
            if len(df['ObstacleDensity'].unique()) > 1:
                density_impact = df.groupby('ObstacleDensity')['ExplorationRate_Mean'].mean()
                best_density = density_impact.idxmax()
                f.write(f"- **æœ€é©éšœå®³ç‰©å¯†åº¦**: {best_density:.3f} (å¹³å‡æ¢æŸ»ç‡: {density_impact[best_density]:.4f})\n")
            
            f.write("\n")
            
            # 6. æ”¹å–„ææ¡ˆ
            f.write("## 6. æ”¹å–„ææ¡ˆ\n\n")
            
            worst_config = exploration_ranking.index[-1]
            worst_rate = exploration_ranking.iloc[-1]
            improvement_potential = best_rate - worst_rate
            
            f.write(f"- Config {worst_config}ã®æ¢æŸ»ç‡ã¯{worst_rate:.4f}ã§ã€æœ€é«˜æ€§èƒ½ã®Config {best_config}ã¨æ¯”ã¹ã¦{improvement_potential:.4f}ã®æ”¹å–„ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚\n")
            f.write(f"- å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ã‚„åˆ†å²ãƒ»çµ±åˆæˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n")
            f.write(f"- éšœå®³ç‰©å¯†åº¦ãŒæ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©³ã—ãåˆ†æã—ã€ç’°å¢ƒé©å¿œæ€§ã‚’å‘ä¸Šã•ã›ã¦ãã ã•ã„ã€‚\n\n")
        
        print(f"âœ“ ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ {report_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‡ºåŠ›
        csv_path = f"{output_dir}/verification_results_summary.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ“ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ {csv_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def run_analysis(self, output_dir: str = "analysis_results"):
        """å®Œå…¨ãªè§£æã‚’å®Ÿè¡Œ"""
        print("ğŸ” ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœè§£æã‚’é–‹å§‹ã—ã¾ã™\n")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.load_results()
        
        if not self.results_data:
            print("âŒ è§£æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. çµ±è¨ˆè¨ˆç®—
        self.calculate_summary_statistics()
        
        # 3. å¯è¦–åŒ–
        self.generate_visualizations(output_dir)
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self.generate_summary_report(output_dir)
        
        print(f"\nğŸ‰ è§£æå®Œäº†ï¼çµæœã¯ {output_dir}/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
        print(f"ğŸ“Š ã‚°ãƒ©ãƒ•: {output_dir}/exploration_analysis.png")
        print(f"ğŸ“ˆ å ±é…¬åˆ†æ: {output_dir}/reward_analysis.png")
        print(f"ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ: {output_dir}/verification_summary_report.md")
        print(f"ğŸ“„ ãƒ‡ãƒ¼ã‚¿: {output_dir}/verification_results_summary.csv")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = VerificationResultAnalyzer()
    analyzer.run_analysis()


if __name__ == "__main__":
    main() 